[2025-02-10 16:03:03,525][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 16:03:03,525][flwr][INFO] - 
[2025-02-10 16:03:03,527][flwr][INFO] - [INIT]
[2025-02-10 16:03:03,527][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 16:03:03,528][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 16:03:09,723][flwr][INFO] - initial parameters (loss, other metrics): 0.07212343354225159, {'accuracy': 0.0957}
[2025-02-10 16:03:09,723][flwr][INFO] - 
[2025-02-10 16:03:09,723][flwr][INFO] - [ROUND 1]
[2025-02-10 16:03:09,723][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:14,327][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:14,343][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:14,404][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:19,421][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:03:23,893][flwr][INFO] - fit progress: (1, 0.07188684873580932, {'accuracy': 0.1473}, 14.170329261003644)
[2025-02-10 16:03:23,897][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:23,990][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:23,990][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:23,991][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:27,209][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:03:27,211][flwr][INFO] - 
[2025-02-10 16:03:27,211][flwr][INFO] - [ROUND 2]
[2025-02-10 16:03:27,212][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:27,266][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:27,266][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:27,267][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:29,918][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:03:33,800][flwr][INFO] - fit progress: (2, 0.07147341668605804, {'accuracy': 0.2316}, 24.07668075800757)
[2025-02-10 16:03:33,800][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:33,877][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:33,878][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:33,879][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:37,307][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:03:37,308][flwr][INFO] - 
[2025-02-10 16:03:37,308][flwr][INFO] - [ROUND 3]
[2025-02-10 16:03:37,308][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:37,385][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:37,385][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:37,386][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:40,015][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:03:43,913][flwr][INFO] - fit progress: (3, 0.07047583551406861, {'accuracy': 0.3063}, 34.189688054000726)
[2025-02-10 16:03:43,913][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:43,999][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:43,999][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:44,000][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:47,323][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:03:47,324][flwr][INFO] - 
[2025-02-10 16:03:47,324][flwr][INFO] - [ROUND 4]
[2025-02-10 16:03:47,324][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:47,403][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:47,403][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:47,406][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:50,032][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:03:53,872][flwr][INFO] - fit progress: (4, 0.06824106695652007, {'accuracy': 0.3462}, 44.14921609900193)
[2025-02-10 16:03:53,872][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:53,917][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:53,917][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:53,918][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:03:56,882][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:03:56,882][flwr][INFO] - 
[2025-02-10 16:03:56,882][flwr][INFO] - [ROUND 5]
[2025-02-10 16:03:56,882][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:03:56,923][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:03:56,923][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:03:56,924][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:00,089][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:04,119][flwr][INFO] - fit progress: (5, 0.06376803604364395, {'accuracy': 0.4029}, 54.39607430700562)
[2025-02-10 16:04:04,119][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:04,236][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:04,236][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:04,237][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:07,426][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:07,426][flwr][INFO] - 
[2025-02-10 16:04:07,426][flwr][INFO] - [ROUND 6]
[2025-02-10 16:04:07,426][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:07,536][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:07,536][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:07,536][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:10,238][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:14,280][flwr][INFO] - fit progress: (6, 0.05624308052062988, {'accuracy': 0.4924}, 64.55719321299694)
[2025-02-10 16:04:14,280][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:14,356][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:14,356][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:14,357][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:17,487][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:17,487][flwr][INFO] - 
[2025-02-10 16:04:17,487][flwr][INFO] - [ROUND 7]
[2025-02-10 16:04:17,487][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:17,562][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:17,562][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:17,563][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:20,296][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:24,089][flwr][INFO] - fit progress: (7, 0.04750582835674286, {'accuracy': 0.5351}, 74.365616340001)
[2025-02-10 16:04:24,089][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:24,174][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:24,174][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:24,175][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:27,796][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:27,796][flwr][INFO] - 
[2025-02-10 16:04:27,797][flwr][INFO] - [ROUND 8]
[2025-02-10 16:04:27,797][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:27,881][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:27,881][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:27,881][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:30,605][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:34,499][flwr][INFO] - fit progress: (8, 0.04210459696054459, {'accuracy': 0.5603}, 84.77558294299524)
[2025-02-10 16:04:34,499][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:34,591][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:34,592][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:34,593][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:37,606][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:37,606][flwr][INFO] - 
[2025-02-10 16:04:37,606][flwr][INFO] - [ROUND 9]
[2025-02-10 16:04:37,607][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:37,697][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:37,699][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:37,700][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:40,315][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:44,438][flwr][INFO] - fit progress: (9, 0.04212694311738014, {'accuracy': 0.5742}, 94.71459757100092)
[2025-02-10 16:04:44,438][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:44,511][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:44,512][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:44,513][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:47,445][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:47,446][flwr][INFO] - 
[2025-02-10 16:04:47,446][flwr][INFO] - [ROUND 10]
[2025-02-10 16:04:47,446][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:47,519][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:47,519][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:47,519][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:50,357][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:04:54,093][flwr][INFO] - fit progress: (10, 0.04755355288386345, {'accuracy': 0.5864}, 104.37010473100236)
[2025-02-10 16:04:54,093][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:54,131][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:54,132][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:54,133][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:57,100][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:04:57,101][flwr][INFO] - 
[2025-02-10 16:04:57,101][flwr][INFO] - [ROUND 11]
[2025-02-10 16:04:57,101][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:04:57,135][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:04:57,137][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:04:57,137][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:04:59,809][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:03,984][flwr][INFO] - fit progress: (11, 0.05789057726264, {'accuracy': 0.5874}, 114.26086486299755)
[2025-02-10 16:05:03,984][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:04,049][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:04,049][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:04,050][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:07,090][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:07,091][flwr][INFO] - 
[2025-02-10 16:05:07,091][flwr][INFO] - [ROUND 12]
[2025-02-10 16:05:07,091][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:07,155][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:07,155][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:07,156][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:09,998][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:13,644][flwr][INFO] - fit progress: (12, 0.0746516761481762, {'accuracy': 0.5803}, 123.92148309899494)
[2025-02-10 16:05:13,645][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:13,667][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:13,667][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:13,668][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:17,152][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:17,152][flwr][INFO] - 
[2025-02-10 16:05:17,152][flwr][INFO] - [ROUND 13]
[2025-02-10 16:05:17,152][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:17,173][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:17,173][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:17,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:19,860][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:23,734][flwr][INFO] - fit progress: (13, 0.0984682781279087, {'accuracy': 0.5672}, 134.01144088500587)
[2025-02-10 16:05:23,735][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:23,785][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:23,785][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:23,786][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:26,643][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:26,643][flwr][INFO] - 
[2025-02-10 16:05:26,643][flwr][INFO] - [ROUND 14]
[2025-02-10 16:05:26,643][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:26,691][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:26,691][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:26,692][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:29,351][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:33,002][flwr][INFO] - fit progress: (14, 0.13106325702667238, {'accuracy': 0.5547}, 143.27913491000072)
[2025-02-10 16:05:33,002][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:33,107][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:33,108][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:33,109][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:36,009][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:36,009][flwr][INFO] - 
[2025-02-10 16:05:36,009][flwr][INFO] - [ROUND 15]
[2025-02-10 16:05:36,010][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:36,112][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:36,112][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:36,113][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:38,819][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:42,482][flwr][INFO] - fit progress: (15, 0.17355224022865295, {'accuracy': 0.5413}, 152.75921207500505)
[2025-02-10 16:05:42,482][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:42,524][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:42,524][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:42,524][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:45,389][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:45,390][flwr][INFO] - 
[2025-02-10 16:05:45,390][flwr][INFO] - [ROUND 16]
[2025-02-10 16:05:45,390][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:45,428][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:45,429][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:45,430][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:48,600][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:05:52,360][flwr][INFO] - fit progress: (16, 0.2281589210987091, {'accuracy': 0.5297}, 162.63708895900345)
[2025-02-10 16:05:52,360][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:52,443][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:52,443][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:52,444][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:55,667][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:05:55,667][flwr][INFO] - 
[2025-02-10 16:05:55,667][flwr][INFO] - [ROUND 17]
[2025-02-10 16:05:55,667][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:05:55,750][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:05:55,750][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:05:55,751][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:05:58,476][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:06:02,184][flwr][INFO] - fit progress: (17, 0.29707495617866514, {'accuracy': 0.5142}, 172.4611333560024)
[2025-02-10 16:06:02,184][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:02,261][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:02,261][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:02,262][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:06,092][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:06:06,092][flwr][INFO] - 
[2025-02-10 16:06:06,092][flwr][INFO] - [ROUND 18]
[2025-02-10 16:06:06,092][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:06,168][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:06,168][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:06,169][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:08,701][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:06:12,340][flwr][INFO] - fit progress: (18, 0.38184761605262757, {'accuracy': 0.5045}, 182.61743604599906)
[2025-02-10 16:06:12,341][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:12,380][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:12,380][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:12,382][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:15,447][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:06:15,448][flwr][INFO] - 
[2025-02-10 16:06:15,448][flwr][INFO] - [ROUND 19]
[2025-02-10 16:06:15,448][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:15,485][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:15,486][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:15,486][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:18,059][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:06:21,851][flwr][INFO] - fit progress: (19, 0.4852946828842163, {'accuracy': 0.4963}, 192.12755092099542)
[2025-02-10 16:06:21,851][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:21,897][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:21,898][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:21,898][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:24,559][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:06:24,559][flwr][INFO] - 
[2025-02-10 16:06:24,559][flwr][INFO] - [ROUND 20]
[2025-02-10 16:06:24,559][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:24,602][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:24,602][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:24,603][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:27,366][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 16:06:31,313][flwr][INFO] - fit progress: (20, 0.6079543823242187, {'accuracy': 0.4886}, 201.59003908500017)
[2025-02-10 16:06:31,313][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:06:31,416][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:06:31,417][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:06:31,418][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: b32caa5fd5888df3d40bd7a07aeef02a8a46d058fe0c3b7eba9e909d) where the task (actor ID: f2d56cb8e6696d3b7c24c78001000000, name=ClientAppActor.__init__, pid=25158, memory used=0.45GB) was running was 7.17GB / 7.45GB (0.962247), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-c01e0e92c2740cc03ea0c087270764f85aa6233d3088f960b04ee126*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
25165	0.58	ray::ClientAppActor.run
25166	0.55	ray::ClientAppActor.run
25164	0.46	ray::ClientAppActor.run
25162	0.46	ray::ClientAppActor.run
25160	0.46	ray::ClientAppActor.run
25163	0.46	ray::ClientAppActor.run
25159	0.45	ray::ClientAppActor.run
25158	0.45	ray::ClientAppActor.run
25161	0.44	ray::ClientAppActor.run
24316	0.34	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=133...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:06:34,220][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 16:06:34,221][flwr][INFO] - 
[2025-02-10 16:06:34,221][flwr][INFO] - [SUMMARY]
[2025-02-10 16:06:34,221][flwr][INFO] - Run finished 20 round(s) in 204.50s
[2025-02-10 16:06:34,226][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 16:06:34,226][flwr][INFO] - 		round 1: 0.07658743328518337
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 2: 0.0762250425638976
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 3: 0.0752919806374444
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 4: 0.07305682080763358
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 5: 0.06845172301486685
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 6: 0.06069329116079542
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 7: 0.05146107508076562
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 8: 0.04543883546634957
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 9: 0.0448585765781226
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 10: 0.0500235597844477
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 11: 0.060419952207141456
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 12: 0.07822511990865072
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 13: 0.10356281476992148
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 14: 0.1387515696110549
[2025-02-10 16:06:34,227][flwr][INFO] - 		round 15: 0.1843090580569373
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 16: 0.2430860960925067
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 17: 0.31689715054300094
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 18: 0.40803861220677695
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 19: 0.5198892527156407
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 20: 0.6521583362861915
[2025-02-10 16:06:34,228][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 0: 0.07212343354225159
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 1: 0.07188684873580932
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 2: 0.07147341668605804
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 3: 0.07047583551406861
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 4: 0.06824106695652007
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 5: 0.06376803604364395
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 6: 0.05624308052062988
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 7: 0.04750582835674286
[2025-02-10 16:06:34,228][flwr][INFO] - 		round 8: 0.04210459696054459
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 9: 0.04212694311738014
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 10: 0.04755355288386345
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 11: 0.05789057726264
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 12: 0.0746516761481762
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 13: 0.0984682781279087
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 14: 0.13106325702667238
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 15: 0.17355224022865295
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 16: 0.2281589210987091
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 17: 0.29707495617866514
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 18: 0.38184761605262757
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 19: 0.4852946828842163
[2025-02-10 16:06:34,229][flwr][INFO] - 		round 20: 0.6079543823242187
[2025-02-10 16:06:34,229][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 16:06:34,229][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867436),
[2025-02-10 16:06:34,229][flwr][INFO] - 	                      (2, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (3, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (4, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (5, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (6, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (7, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (8, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (9, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (10, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (11, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (12, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (13, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (14, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (15, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (16, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (17, 0.6813471387867436),
[2025-02-10 16:06:34,230][flwr][INFO] - 	                      (18, 0.6813471387867436),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                      (19, 0.6813471387867436),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                      (20, 0.6813471387867436)],
[2025-02-10 16:06:34,231][flwr][INFO] - 	 'clients_losses': [(1, 2.3018557054025157),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (2, 2.2922315261982105),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (3, 2.2738813523893002),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (4, 2.231418325282909),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (5, 2.141906660574454),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (6, 1.9753065948133115),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (7, 1.7334030584052758),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (8, 1.515561788170426),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (9, 1.446538852320777),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (10, 1.5576195421042265),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (11, 1.837235732873281),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (12, 2.302966966452422),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (13, 2.988076771630181),
[2025-02-10 16:06:34,231][flwr][INFO] - 	                    (14, 3.9352153407202826),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (15, 5.198585812250773),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (16, 6.838367889545582),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (17, 8.921875349680583),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (18, 11.49800987067046),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (19, 14.620772125102855),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                    (20, 18.372580302203144)]}
[2025-02-10 16:06:34,232][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 16:06:34,232][flwr][INFO] - 	{'clients_accuracies': [(1, 0.14259259259259258),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (2, 0.22499999999999998),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (3, 0.29166666666666663),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (4, 0.3287037037037037),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (5, 0.40185185185185185),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (6, 0.5),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (7, 0.5287037037037037),
[2025-02-10 16:06:34,232][flwr][INFO] - 	                        (8, 0.5592592592592592),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (9, 0.5675925925925926),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (10, 0.587037037037037),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (11, 0.5842592592592593),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (12, 0.5777777777777778),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (13, 0.5666666666666668),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (14, 0.55),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (15, 0.5342592592592593),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (16, 0.5296296296296296),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (17, 0.5175925925925925),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (18, 0.5055555555555555),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (19, 0.499074074074074),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                        (20, 0.48518518518518516)],
[2025-02-10 16:06:34,233][flwr][INFO] - 	 'clients_losses': [(1, 0.07658743328518337),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                    (2, 0.0762250425638976),
[2025-02-10 16:06:34,233][flwr][INFO] - 	                    (3, 0.0752919806374444),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (4, 0.07305682080763358),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (5, 0.06845172301486685),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (6, 0.06069329116079542),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (7, 0.05146107508076562),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (8, 0.04543883546634957),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (9, 0.0448585765781226),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (10, 0.0500235597844477),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (11, 0.060419952207141456),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (12, 0.07822511990865072),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (13, 0.10356281476992148),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (14, 0.1387515696110549),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (15, 0.1843090580569373),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (16, 0.2430860960925067),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (17, 0.31689715054300094),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (18, 0.40803861220677695),
[2025-02-10 16:06:34,234][flwr][INFO] - 	                    (19, 0.5198892527156407),
[2025-02-10 16:06:34,235][flwr][INFO] - 	                    (20, 0.6521583362861915)]}
[2025-02-10 16:06:34,235][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 16:06:34,235][flwr][INFO] - 	{'accuracy': [(0, 0.0957),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (1, 0.1473),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (2, 0.2316),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (3, 0.3063),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (4, 0.3462),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (5, 0.4029),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (6, 0.4924),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (7, 0.5351),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (8, 0.5603),
[2025-02-10 16:06:34,235][flwr][INFO] - 	              (9, 0.5742),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (10, 0.5864),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (11, 0.5874),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (12, 0.5803),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (13, 0.5672),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (14, 0.5547),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (15, 0.5413),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (16, 0.5297),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (17, 0.5142),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (18, 0.5045),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (19, 0.4963),
[2025-02-10 16:06:34,236][flwr][INFO] - 	              (20, 0.4886)]}
[2025-02-10 16:06:34,236][flwr][INFO] - 
