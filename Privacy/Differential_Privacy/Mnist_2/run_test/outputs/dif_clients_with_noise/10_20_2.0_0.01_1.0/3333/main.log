[2025-02-10 15:36:20,075][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:36:20,075][flwr][INFO] - 
[2025-02-10 15:36:20,077][flwr][INFO] - [INIT]
[2025-02-10 15:36:20,077][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:36:20,077][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:36:26,542][flwr][INFO] - initial parameters (loss, other metrics): 0.07210463590621949, {'accuracy': 0.0953}
[2025-02-10 15:36:26,542][flwr][INFO] - 
[2025-02-10 15:36:26,542][flwr][INFO] - [ROUND 1]
[2025-02-10 15:36:26,543][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:36:32,150][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:36:32,196][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:36:32,331][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:36:37,043][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:36:41,789][flwr][INFO] - fit progress: (1, 0.07188884823322296, {'accuracy': 0.0834}, 15.246817999999621)
[2025-02-10 15:36:41,793][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:36:41,903][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:36:41,903][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:36:41,905][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:36:44,503][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:36:44,504][flwr][INFO] - 
[2025-02-10 15:36:44,505][flwr][INFO] - [ROUND 2]
[2025-02-10 15:36:44,505][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:36:44,577][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:36:44,577][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:36:44,578][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:36:47,215][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:36:51,301][flwr][INFO] - fit progress: (2, 0.07152842376232148, {'accuracy': 0.219}, 24.75851240499469)
[2025-02-10 15:36:51,301][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:36:51,389][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:36:51,390][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:36:51,390][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:36:54,610][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:36:54,610][flwr][INFO] - 
[2025-02-10 15:36:54,611][flwr][INFO] - [ROUND 3]
[2025-02-10 15:36:54,613][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:36:54,696][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:36:54,696][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:36:54,697][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:36:57,423][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:01,462][flwr][INFO] - fit progress: (3, 0.07037352857589722, {'accuracy': 0.2472}, 34.919705465988955)
[2025-02-10 15:37:01,462][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:01,510][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:01,510][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:01,511][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:05,170][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:05,170][flwr][INFO] - 
[2025-02-10 15:37:05,170][flwr][INFO] - [ROUND 4]
[2025-02-10 15:37:05,170][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:05,217][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:05,217][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:05,218][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:07,779][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:11,764][flwr][INFO] - fit progress: (4, 0.06738070952892304, {'accuracy': 0.3379}, 45.22201606100134)
[2025-02-10 15:37:11,765][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:11,829][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:11,831][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:11,832][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:14,671][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:14,672][flwr][INFO] - 
[2025-02-10 15:37:14,672][flwr][INFO] - [ROUND 5]
[2025-02-10 15:37:14,672][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:14,739][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:14,739][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:14,740][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:17,382][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:21,015][flwr][INFO] - fit progress: (5, 0.06168172039985657, {'accuracy': 0.4105}, 54.47275763298967)
[2025-02-10 15:37:21,015][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:21,149][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:21,149][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:21,150][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:24,442][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:24,442][flwr][INFO] - 
[2025-02-10 15:37:24,443][flwr][INFO] - [ROUND 6]
[2025-02-10 15:37:24,443][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:24,555][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:24,556][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:24,559][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:27,056][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:30,960][flwr][INFO] - fit progress: (6, 0.0534296387553215, {'accuracy': 0.4717}, 64.41762566799298)
[2025-02-10 15:37:30,960][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:31,071][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:31,071][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:31,072][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:33,969][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:33,970][flwr][INFO] - 
[2025-02-10 15:37:33,970][flwr][INFO] - [ROUND 7]
[2025-02-10 15:37:33,970][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:34,077][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:34,078][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:34,078][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:36,680][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:40,312][flwr][INFO] - fit progress: (7, 0.04472587530016899, {'accuracy': 0.5301}, 73.76960912799404)
[2025-02-10 15:37:40,312][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:40,388][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:40,389][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:40,390][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:43,418][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:43,419][flwr][INFO] - 
[2025-02-10 15:37:43,419][flwr][INFO] - [ROUND 8]
[2025-02-10 15:37:43,419][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:43,496][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:43,496][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:43,497][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:46,028][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:49,939][flwr][INFO] - fit progress: (8, 0.038629722315073015, {'accuracy': 0.5859}, 83.39622911599872)
[2025-02-10 15:37:49,939][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:50,007][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:50,007][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:50,007][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:53,346][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:37:53,346][flwr][INFO] - 
[2025-02-10 15:37:53,346][flwr][INFO] - [ROUND 9]
[2025-02-10 15:37:53,346][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:53,415][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:53,416][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:53,416][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:37:56,053][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:37:59,829][flwr][INFO] - fit progress: (9, 0.038218641132116316, {'accuracy': 0.6127}, 93.2865158939967)
[2025-02-10 15:37:59,829][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:37:59,927][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:37:59,927][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:37:59,928][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:02,638][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:02,638][flwr][INFO] - 
[2025-02-10 15:38:02,638][flwr][INFO] - [ROUND 10]
[2025-02-10 15:38:02,638][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:02,732][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:02,733][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:02,733][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:05,349][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:09,008][flwr][INFO] - fit progress: (10, 0.04273337041735649, {'accuracy': 0.6271}, 102.46549467599834)
[2025-02-10 15:38:09,008][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:09,045][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:09,046][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:09,046][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:12,215][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:12,215][flwr][INFO] - 
[2025-02-10 15:38:12,215][flwr][INFO] - [ROUND 11]
[2025-02-10 15:38:12,215][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:12,250][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:12,250][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:12,251][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:14,923][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:18,539][flwr][INFO] - fit progress: (11, 0.05246819906234741, {'accuracy': 0.6316}, 111.9969634799927)
[2025-02-10 15:38:18,539][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:18,564][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:18,565][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:18,565][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:21,547][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:21,547][flwr][INFO] - 
[2025-02-10 15:38:21,547][flwr][INFO] - [ROUND 12]
[2025-02-10 15:38:21,547][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:21,568][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:21,568][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:21,569][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:24,155][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:28,194][flwr][INFO] - fit progress: (12, 0.06749428218603135, {'accuracy': 0.6284}, 121.6520361139992)
[2025-02-10 15:38:28,195][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:28,279][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:28,279][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:28,280][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:31,101][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:31,101][flwr][INFO] - 
[2025-02-10 15:38:31,101][flwr][INFO] - [ROUND 13]
[2025-02-10 15:38:31,102][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:31,185][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:31,186][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:31,187][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:33,809][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:37,453][flwr][INFO] - fit progress: (13, 0.08892750688791275, {'accuracy': 0.6174}, 130.9105614729924)
[2025-02-10 15:38:37,453][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:37,499][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:37,499][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:37,500][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:40,259][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:40,260][flwr][INFO] - 
[2025-02-10 15:38:40,260][flwr][INFO] - [ROUND 14]
[2025-02-10 15:38:40,260][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:40,305][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:40,305][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:40,306][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:42,868][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:46,583][flwr][INFO] - fit progress: (14, 0.11769104089736938, {'accuracy': 0.6042}, 140.04048959098873)
[2025-02-10 15:38:46,583][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:46,616][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:46,616][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:46,617][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:49,790][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:49,790][flwr][INFO] - 
[2025-02-10 15:38:49,790][flwr][INFO] - [ROUND 15]
[2025-02-10 15:38:49,791][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:49,822][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:49,823][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:49,824][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:52,497][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:38:56,229][flwr][INFO] - fit progress: (15, 0.15589646835327148, {'accuracy': 0.5927}, 149.68649583599472)
[2025-02-10 15:38:56,229][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:56,338][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:56,339][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:56,340][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:38:59,138][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:38:59,138][flwr][INFO] - 
[2025-02-10 15:38:59,138][flwr][INFO] - [ROUND 16]
[2025-02-10 15:38:59,138][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:38:59,241][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:38:59,241][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:38:59,242][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:01,946][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:39:05,853][flwr][INFO] - fit progress: (16, 0.2044461536169052, {'accuracy': 0.576}, 159.310915568989)
[2025-02-10 15:39:05,853][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:05,954][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:05,954][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:05,955][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:08,860][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:39:08,861][flwr][INFO] - 
[2025-02-10 15:39:08,861][flwr][INFO] - [ROUND 17]
[2025-02-10 15:39:08,861][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:08,960][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:08,960][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:08,961][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:11,567][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:39:15,445][flwr][INFO] - fit progress: (17, 0.26545533447265623, {'accuracy': 0.5629}, 168.90223887599132)
[2025-02-10 15:39:15,445][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:15,472][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:15,473][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:15,473][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:18,852][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:39:18,852][flwr][INFO] - 
[2025-02-10 15:39:18,852][flwr][INFO] - [ROUND 18]
[2025-02-10 15:39:18,852][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:18,877][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:18,878][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:18,878][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:21,760][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:39:25,852][flwr][INFO] - fit progress: (18, 0.3400026909351349, {'accuracy': 0.5496}, 179.3100282099913)
[2025-02-10 15:39:25,853][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:25,889][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:25,890][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:25,890][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:28,260][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:39:28,260][flwr][INFO] - 
[2025-02-10 15:39:28,261][flwr][INFO] - [ROUND 19]
[2025-02-10 15:39:28,261][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:28,294][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:28,295][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:28,296][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:30,969][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:39:34,647][flwr][INFO] - fit progress: (19, 0.43238316736221316, {'accuracy': 0.5385}, 188.10496625700034)
[2025-02-10 15:39:34,647][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:34,706][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:34,707][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:34,707][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:38,156][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:39:38,156][flwr][INFO] - 
[2025-02-10 15:39:38,157][flwr][INFO] - [ROUND 20]
[2025-02-10 15:39:38,157][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:38,213][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:38,214][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:38,216][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:40,864][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:39:44,524][flwr][INFO] - fit progress: (20, 0.5439741333961486, {'accuracy': 0.526}, 197.9821396849875)
[2025-02-10 15:39:44,525][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:39:44,626][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:39:44,627][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:39:44,627][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: d3ef61c5e63a720060a2023627dee0fd608f5f3a997a7846c5f18145) where the task (actor ID: 7d68555a13a4a31a5185fd8801000000, name=ClientAppActor.__init__, pid=14896, memory used=0.45GB) was running was 7.36GB / 7.45GB (0.988255), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-75f271580921ce4be0abbdd9e0630d4699146115617fa284fc2a6661*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
14895	0.53	ray::ClientAppActor.run
14894	0.53	ray::ClientAppActor.run
14889	0.52	ray::ClientAppActor.run
14893	0.49	ray::ClientAppActor.run
14892	0.49	ray::ClientAppActor.run
14890	0.48	ray::ClientAppActor.run
14888	0.48	ray::ClientAppActor.run
14896	0.45	ray::ClientAppActor.run
14883	0.44	ray::ClientAppActor.run
14891	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:39:47,631][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:39:47,632][flwr][INFO] - 
[2025-02-10 15:39:47,632][flwr][INFO] - [SUMMARY]
[2025-02-10 15:39:47,633][flwr][INFO] - Run finished 20 round(s) in 201.09s
[2025-02-10 15:39:47,639][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 1: 0.07654259381470857
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 2: 0.07618941267331442
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 3: 0.07505191078892462
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 4: 0.07205009791586134
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 5: 0.0659929175067831
[2025-02-10 15:39:47,639][flwr][INFO] - 		round 6: 0.058281695511606
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 7: 0.050001006545843905
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 8: 0.044253034503371626
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 9: 0.04425277588544069
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 10: 0.04958506541119682
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 11: 0.060627274767116254
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 12: 0.07761067383819156
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 13: 0.10217499766084884
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 14: 0.13519918852382234
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 15: 0.178578382730484
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 16: 0.23344937474639327
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 17: 0.3011980215708415
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 18: 0.38435604793054085
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 19: 0.4882459658163565
[2025-02-10 15:39:47,640][flwr][INFO] - 		round 20: 0.6131133256135164
[2025-02-10 15:39:47,641][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 0: 0.07210463590621949
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 1: 0.07188884823322296
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 2: 0.07152842376232148
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 3: 0.07037352857589722
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 4: 0.06738070952892304
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 5: 0.06168172039985657
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 6: 0.0534296387553215
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 7: 0.04472587530016899
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 8: 0.038629722315073015
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 9: 0.038218641132116316
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 10: 0.04273337041735649
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 11: 0.05246819906234741
[2025-02-10 15:39:47,641][flwr][INFO] - 		round 12: 0.06749428218603135
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 13: 0.08892750688791275
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 14: 0.11769104089736938
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 15: 0.15589646835327148
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 16: 0.2044461536169052
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 17: 0.26545533447265623
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 18: 0.3400026909351349
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 19: 0.43238316736221316
[2025-02-10 15:39:47,642][flwr][INFO] - 		round 20: 0.5439741333961486
[2025-02-10 15:39:47,642][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:39:47,642][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867436),
[2025-02-10 15:39:47,642][flwr][INFO] - 	                      (2, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (3, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (4, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (5, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (6, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (7, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (8, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (9, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (10, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (11, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (12, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (13, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (14, 0.6813471387867436),
[2025-02-10 15:39:47,643][flwr][INFO] - 	                      (15, 0.6813471387867436),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                      (16, 0.6813471387867436),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                      (17, 0.6813471387867436),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                      (18, 0.6813471387867436),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                      (19, 0.6813471387867436),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                      (20, 0.6813471387867436)],
[2025-02-10 15:39:47,644][flwr][INFO] - 	 'clients_losses': [(1, 2.3007746873078525),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (2, 2.2934231369583693),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (3, 2.2742476940155028),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (4, 2.2208748446570503),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (5, 2.1010897442146583),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (6, 1.9015154008512143),
[2025-02-10 15:39:47,644][flwr][INFO] - 	                    (7, 1.6517210174489905),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (8, 1.4325886571848834),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (9, 1.325429364486977),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (10, 1.3853048611570287),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (11, 1.6096400030233242),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (12, 2.005436078155482),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (13, 2.5832100385868992),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (14, 3.381424166538097),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (15, 4.447294896620291),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (16, 5.835539214257841),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (17, 7.5903891157220915),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (18, 9.78940953678555),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (19, 12.490704649466055),
[2025-02-10 15:39:47,645][flwr][INFO] - 	                    (20, 15.779990379898637)]}
[2025-02-10 15:39:47,646][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:39:47,646][flwr][INFO] - 	{'clients_accuracies': [(1, 0.08796296296296297),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (2, 0.21203703703703705),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (3, 0.24814814814814815),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (4, 0.33055555555555555),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (5, 0.4),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (6, 0.4416666666666667),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (7, 0.4972222222222222),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (8, 0.550925925925926),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (9, 0.575),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (10, 0.6),
[2025-02-10 15:39:47,646][flwr][INFO] - 	                        (11, 0.6203703703703703),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (12, 0.6231481481481481),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (13, 0.6194444444444444),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (14, 0.6083333333333334),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (15, 0.5944444444444446),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (16, 0.5833333333333334),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (17, 0.5657407407407408),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (18, 0.5407407407407407),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (19, 0.5222222222222223),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                        (20, 0.5138888888888888)],
[2025-02-10 15:39:47,647][flwr][INFO] - 	 'clients_losses': [(1, 0.07654259381470857),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                    (2, 0.07618941267331442),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                    (3, 0.07505191078892462),
[2025-02-10 15:39:47,647][flwr][INFO] - 	                    (4, 0.07205009791586134),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (5, 0.0659929175067831),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (6, 0.058281695511606),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (7, 0.050001006545843905),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (8, 0.044253034503371626),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (9, 0.04425277588544069),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (10, 0.04958506541119682),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (11, 0.060627274767116254),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (12, 0.07761067383819156),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (13, 0.10217499766084884),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (14, 0.13519918852382234),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (15, 0.178578382730484),
[2025-02-10 15:39:47,648][flwr][INFO] - 	                    (16, 0.23344937474639327),
[2025-02-10 15:39:47,649][flwr][INFO] - 	                    (17, 0.3011980215708415),
[2025-02-10 15:39:47,649][flwr][INFO] - 	                    (18, 0.38435604793054085),
[2025-02-10 15:39:47,649][flwr][INFO] - 	                    (19, 0.4882459658163565),
[2025-02-10 15:39:47,649][flwr][INFO] - 	                    (20, 0.6131133256135164)]}
[2025-02-10 15:39:47,649][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:39:47,649][flwr][INFO] - 	{'accuracy': [(0, 0.0953),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (1, 0.0834),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (2, 0.219),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (3, 0.2472),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (4, 0.3379),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (5, 0.4105),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (6, 0.4717),
[2025-02-10 15:39:47,649][flwr][INFO] - 	              (7, 0.5301),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (8, 0.5859),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (9, 0.6127),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (10, 0.6271),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (11, 0.6316),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (12, 0.6284),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (13, 0.6174),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (14, 0.6042),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (15, 0.5927),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (16, 0.576),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (17, 0.5629),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (18, 0.5496),
[2025-02-10 15:39:47,650][flwr][INFO] - 	              (19, 0.5385),
[2025-02-10 15:39:47,651][flwr][INFO] - 	              (20, 0.526)]}
[2025-02-10 15:39:47,651][flwr][INFO] - 
