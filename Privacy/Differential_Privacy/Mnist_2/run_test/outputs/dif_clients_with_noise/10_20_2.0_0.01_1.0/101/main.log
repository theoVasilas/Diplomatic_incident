[2025-02-10 15:28:32,953][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:28:32,954][flwr][INFO] - 
[2025-02-10 15:28:32,956][flwr][INFO] - [INIT]
[2025-02-10 15:28:32,956][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:28:32,956][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:28:39,047][flwr][INFO] - initial parameters (loss, other metrics): 0.07228001170158387, {'accuracy': 0.1135}
[2025-02-10 15:28:39,048][flwr][INFO] - 
[2025-02-10 15:28:39,048][flwr][INFO] - [ROUND 1]
[2025-02-10 15:28:39,048][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:28:44,363][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:44,472][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:44,896][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:44,902][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:45,007][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:28:45,007][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:28:50,241][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:28:55,119][flwr][INFO] - fit progress: (1, 0.07207292003631592, {'accuracy': 0.1136}, 16.070968721003737)
[2025-02-10 15:28:55,122][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:28:55,242][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:55,244][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:55,245][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:28:55,245][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:55,246][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:55,247][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:28:58,440][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:28:58,442][flwr][INFO] - 
[2025-02-10 15:28:58,442][flwr][INFO] - [ROUND 2]
[2025-02-10 15:28:58,443][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:28:58,520][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:58,521][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:58,523][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:28:58,524][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:28:58,524][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:28:58,525][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:01,053][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:05,128][flwr][INFO] - fit progress: (2, 0.07181703362464904, {'accuracy': 0.1704}, 26.080681284001912)
[2025-02-10 15:29:05,128][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:05,236][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:05,236][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:05,238][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:05,239][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:05,239][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:05,240][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:08,136][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:08,136][flwr][INFO] - 
[2025-02-10 15:29:08,137][flwr][INFO] - [ROUND 3]
[2025-02-10 15:29:08,137][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:08,242][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:08,242][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:08,243][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:08,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:08,244][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:08,244][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:10,747][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:14,587][flwr][INFO] - fit progress: (3, 0.07101019868850708, {'accuracy': 0.2571}, 35.53961807899759)
[2025-02-10 15:29:14,587][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:14,653][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:14,655][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:14,655][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:14,656][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:14,656][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:14,657][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:18,196][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:18,196][flwr][INFO] - 
[2025-02-10 15:29:18,196][flwr][INFO] - [ROUND 4]
[2025-02-10 15:29:18,196][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:18,262][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:18,263][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:18,263][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:18,263][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:18,264][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:18,264][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:20,810][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:24,641][flwr][INFO] - fit progress: (4, 0.06893159718513489, {'accuracy': 0.2668}, 45.59301156899892)
[2025-02-10 15:29:24,641][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:24,673][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:24,674][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:24,675][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:24,676][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:24,676][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:24,677][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:27,347][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:27,348][flwr][INFO] - 
[2025-02-10 15:29:27,348][flwr][INFO] - [ROUND 5]
[2025-02-10 15:29:27,348][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:27,379][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:27,379][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:27,380][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:27,381][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:27,381][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:27,382][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:29,959][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:33,772][flwr][INFO] - fit progress: (5, 0.0647016816496849, {'accuracy': 0.3311}, 54.724674665994826)
[2025-02-10 15:29:33,772][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:33,821][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:33,821][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:33,822][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:33,910][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:33,910][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:33,910][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:36,379][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:36,380][flwr][INFO] - 
[2025-02-10 15:29:36,380][flwr][INFO] - [ROUND 6]
[2025-02-10 15:29:36,380][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:36,426][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:36,426][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:36,426][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:36,427][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:36,427][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:36,428][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:38,988][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:43,271][flwr][INFO] - fit progress: (6, 0.05736381002664566, {'accuracy': 0.4706}, 64.22320848499658)
[2025-02-10 15:29:43,271][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:43,336][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:43,337][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:43,339][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:43,339][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:43,340][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:43,341][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:45,977][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:45,977][flwr][INFO] - 
[2025-02-10 15:29:45,978][flwr][INFO] - [ROUND 7]
[2025-02-10 15:29:45,978][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:46,043][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:46,044][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:46,044][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:46,045][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:46,045][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:46,046][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:48,685][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:29:52,496][flwr][INFO] - fit progress: (7, 0.04868838189840317, {'accuracy': 0.512}, 73.44878345400502)
[2025-02-10 15:29:52,497][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:52,555][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:52,555][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:52,555][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:52,556][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:52,555][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:52,557][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:55,403][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:29:55,404][flwr][INFO] - 
[2025-02-10 15:29:55,404][flwr][INFO] - [ROUND 8]
[2025-02-10 15:29:55,404][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:29:55,459][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:55,461][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:55,462][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:29:55,462][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:29:55,462][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:55,463][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:29:58,013][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:02,132][flwr][INFO] - fit progress: (8, 0.0448625083565712, {'accuracy': 0.5226}, 83.08480033899832)
[2025-02-10 15:30:02,133][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:02,172][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:02,174][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:02,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:02,173][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:02,175][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:02,175][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:04,839][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:04,839][flwr][INFO] - 
[2025-02-10 15:30:04,839][flwr][INFO] - [ROUND 9]
[2025-02-10 15:30:04,840][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:04,878][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:04,878][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:04,878][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:04,880][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:04,880][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:04,881][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:07,348][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:11,403][flwr][INFO] - fit progress: (9, 0.04762651956677437, {'accuracy': 0.5359}, 92.35574296599953)
[2025-02-10 15:30:11,404][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:11,491][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:11,492][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:11,492][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:11,492][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:11,493][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:11,493][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:14,510][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:14,510][flwr][INFO] - 
[2025-02-10 15:30:14,510][flwr][INFO] - [ROUND 10]
[2025-02-10 15:30:14,510][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:14,597][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:14,597][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:14,598][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:14,599][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:14,599][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:14,600][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:17,119][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:20,940][flwr][INFO] - fit progress: (10, 0.05797681639790535, {'accuracy': 0.5382}, 101.89287924999371)
[2025-02-10 15:30:20,941][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:21,008][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:21,009][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:21,010][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:21,011][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:21,011][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:21,012][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:23,949][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:23,949][flwr][INFO] - 
[2025-02-10 15:30:23,949][flwr][INFO] - [ROUND 11]
[2025-02-10 15:30:23,949][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:24,014][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:24,016][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:24,016][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:24,017][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:24,016][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:24,018][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:26,556][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:31,578][flwr][INFO] - fit progress: (11, 0.07520018348693848, {'accuracy': 0.5393}, 112.53075987899501)
[2025-02-10 15:30:31,579][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:31,625][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:31,625][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:31,627][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:31,628][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:31,629][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:31,629][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:33,986][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:33,986][flwr][INFO] - 
[2025-02-10 15:30:33,986][flwr][INFO] - [ROUND 12]
[2025-02-10 15:30:33,986][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:34,032][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:34,033][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:34,033][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:34,033][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:34,034][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:34,035][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:36,595][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:40,223][flwr][INFO] - fit progress: (12, 0.10105440009832382, {'accuracy': 0.5288}, 121.17518829999608)
[2025-02-10 15:30:40,223][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:40,245][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:40,245][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:40,246][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:40,246][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:40,246][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:40,247][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:42,929][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:42,929][flwr][INFO] - 
[2025-02-10 15:30:42,929][flwr][INFO] - [ROUND 13]
[2025-02-10 15:30:42,930][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:42,946][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:42,949][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:42,949][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:42,950][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:42,950][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:42,951][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:46,338][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:49,887][flwr][INFO] - fit progress: (13, 0.13656954898834228, {'accuracy': 0.5156}, 130.83920939799282)
[2025-02-10 15:30:49,887][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:49,959][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:49,960][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:49,960][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:49,961][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:49,962][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:49,963][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:52,693][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:30:52,694][flwr][INFO] - 
[2025-02-10 15:30:52,694][flwr][INFO] - [ROUND 14]
[2025-02-10 15:30:52,694][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:52,766][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:52,767][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:52,768][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:52,768][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:52,769][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:52,769][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:55,500][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:30:59,077][flwr][INFO] - fit progress: (14, 0.18340807065963746, {'accuracy': 0.5049}, 140.02975889199297)
[2025-02-10 15:30:59,078][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:30:59,176][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:59,177][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:59,178][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:30:59,179][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:30:59,179][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:30:59,180][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:01,484][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:01,484][flwr][INFO] - 
[2025-02-10 15:31:01,484][flwr][INFO] - [ROUND 15]
[2025-02-10 15:31:01,484][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:01,584][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:01,584][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:01,584][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:01,585][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:01,585][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:01,586][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:04,195][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:31:07,855][flwr][INFO] - fit progress: (15, 0.24435087838172911, {'accuracy': 0.4903}, 148.80717020899465)
[2025-02-10 15:31:07,855][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:07,894][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:07,894][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:07,895][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:07,896][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:07,896][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:07,896][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:10,963][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:10,963][flwr][INFO] - 
[2025-02-10 15:31:10,963][flwr][INFO] - [ROUND 16]
[2025-02-10 15:31:10,964][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:11,000][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:11,001][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:11,001][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:11,002][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:11,003][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:11,003][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:13,570][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:31:17,185][flwr][INFO] - fit progress: (16, 0.3211669630527496, {'accuracy': 0.4762}, 158.13711451200652)
[2025-02-10 15:31:17,185][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:17,211][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:17,213][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:17,213][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:17,212][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:17,214][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:17,214][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:24,598][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:24,599][flwr][INFO] - 
[2025-02-10 15:31:24,599][flwr][INFO] - [ROUND 17]
[2025-02-10 15:31:24,599][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:24,624][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:24,624][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:24,625][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:24,625][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:24,626][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:24,627][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:28,106][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:31:32,321][flwr][INFO] - fit progress: (17, 0.4167615206241608, {'accuracy': 0.4617}, 173.27366254900699)
[2025-02-10 15:31:32,321][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:32,433][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:32,434][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:32,434][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:32,435][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:32,436][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:32,436][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:35,835][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:35,835][flwr][INFO] - 
[2025-02-10 15:31:35,835][flwr][INFO] - [ROUND 18]
[2025-02-10 15:31:35,836][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:35,943][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:35,944][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:35,944][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:35,945][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:35,945][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:35,946][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:38,642][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:31:42,268][flwr][INFO] - fit progress: (18, 0.5340760211467743, {'accuracy': 0.4505}, 183.22075505400426)
[2025-02-10 15:31:42,269][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:42,352][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:42,355][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:42,355][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:42,355][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:42,356][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:42,357][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:45,077][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:45,077][flwr][INFO] - 
[2025-02-10 15:31:45,077][flwr][INFO] - [ROUND 19]
[2025-02-10 15:31:45,077][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:45,159][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:45,161][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:45,161][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:45,160][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:45,162][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:45,163][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:47,785][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:31:51,338][flwr][INFO] - fit progress: (19, 0.675135165977478, {'accuracy': 0.4412}, 192.29079647999606)
[2025-02-10 15:31:51,339][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:51,371][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:51,373][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:51,373][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:51,374][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:51,373][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:51,375][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:53,947][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:31:53,947][flwr][INFO] - 
[2025-02-10 15:31:53,947][flwr][INFO] - [ROUND 20]
[2025-02-10 15:31:53,947][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:31:53,977][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:53,978][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:53,978][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:31:53,978][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:31:53,978][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:53,979][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:31:56,554][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:32:00,149][flwr][INFO] - fit progress: (20, 0.8448495427131653, {'accuracy': 0.429}, 201.1012649100012)
[2025-02-10 15:32:00,149][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:32:00,187][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:00,187][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:00,188][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: 015545388bd8b50696f31ebf01000000, name=ClientAppActor.__init__, pid=2900, memory used=0.56GB) was running was 7.36GB / 7.45GB (0.987507), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-46ac04fd7f10952f2afd814c8ede4d610dc768a8e42ba194f09ef30f*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
2899	0.56	ray::ClientAppActor.run
2900	0.56	ray::ClientAppActor.run
2898	0.56	ray::ClientAppActor.run
2902	0.56	ray::ClientAppActor.run
2901	0.56	ray::ClientAppActor.run
2896	0.56	ray::ClientAppActor.run
2897	0.54	ray::ClientAppActor.run
2894	0.54	ray::ClientAppActor.run
2893	0.53	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:00,189][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:00,189][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:00,190][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: a351be96a4c294ef7aa25d1420bb4f6ab77451e754681869dd2efc58) where the task (actor ID: d4cc93b492398e025982026401000000, name=ClientAppActor.__init__, pid=2895, memory used=0.36GB) was running was 7.17GB / 7.45GB (0.962524), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-2f1f2b663a559e48598dbda0a2b2e6aa96893a2b8740392271a0fab3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.72	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
588	0.51	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
2902	0.45	ray::ClientAppActor.run
2900	0.45	ray::ClientAppActor.run
2901	0.45	ray::ClientAppActor.run
2899	0.45	ray::ClientAppActor.run
2065	0.39	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=101...
2897	0.36	ray::ClientAppActor.run
2895	0.36	ray::ClientAppActor.run
2896	0.36	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:03,058][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:32:03,059][flwr][INFO] - 
[2025-02-10 15:32:03,059][flwr][INFO] - [SUMMARY]
[2025-02-10 15:32:03,059][flwr][INFO] - Run finished 20 round(s) in 204.01s
[2025-02-10 15:32:03,065][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:32:03,065][flwr][INFO] - 		round 1: 0.0766977918644746
[2025-02-10 15:32:03,065][flwr][INFO] - 		round 2: 0.07641112556060156
[2025-02-10 15:32:03,065][flwr][INFO] - 		round 3: 0.07554939364393552
[2025-02-10 15:32:03,065][flwr][INFO] - 		round 4: 0.07334720542033514
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 5: 0.0690167294194301
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 6: 0.061200745527942975
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 7: 0.05209258235991002
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 8: 0.04823511689901352
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 9: 0.05190100185573101
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 10: 0.06413151832918326
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 11: 0.08425165874262651
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 12: 0.11429199439783891
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 13: 0.1556917409102122
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 14: 0.2098839377363523
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 15: 0.27981051007906593
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 16: 0.3675472925106684
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 17: 0.47633898456891377
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 18: 0.6080653558174769
[2025-02-10 15:32:03,066][flwr][INFO] - 		round 19: 0.7676865657170613
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 20: 0.9610905567804973
[2025-02-10 15:32:03,067][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 0: 0.07228001170158387
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 1: 0.07207292003631592
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 2: 0.07181703362464904
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 3: 0.07101019868850708
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 4: 0.06893159718513489
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 5: 0.0647016816496849
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 6: 0.05736381002664566
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 7: 0.04868838189840317
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 8: 0.0448625083565712
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 9: 0.04762651956677437
[2025-02-10 15:32:03,067][flwr][INFO] - 		round 10: 0.05797681639790535
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 11: 0.07520018348693848
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 12: 0.10105440009832382
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 13: 0.13656954898834228
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 14: 0.18340807065963746
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 15: 0.24435087838172911
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 16: 0.3211669630527496
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 17: 0.4167615206241608
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 18: 0.5340760211467743
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 19: 0.675135165977478
[2025-02-10 15:32:03,068][flwr][INFO] - 		round 20: 0.8448495427131653
[2025-02-10 15:32:03,068][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:32:03,068][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867437),
[2025-02-10 15:32:03,068][flwr][INFO] - 	                      (2, 0.6813471387867437),
[2025-02-10 15:32:03,068][flwr][INFO] - 	                      (3, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (4, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (5, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (6, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (7, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (8, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (9, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (10, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (11, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (12, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (13, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (14, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (15, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (16, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (17, 0.6813471387867437),
[2025-02-10 15:32:03,069][flwr][INFO] - 	                      (18, 0.6813471387867437),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                      (19, 0.6813471387867437),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                      (20, 0.6813471387867437)],
[2025-02-10 15:32:03,070][flwr][INFO] - 	 'clients_losses': [(1, 2.3071276346842446),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (2, 2.300387038787206),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (3, 2.2869336704413095),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (4, 2.247730745871862),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (5, 2.1584068447351457),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (6, 1.990211339791616),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (7, 1.7312748124202093),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (8, 1.501056463519732),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (9, 1.4582830250263215),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (10, 1.6393773203094801),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (11, 2.042449474334717),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (12, 2.678271002570788),
[2025-02-10 15:32:03,070][flwr][INFO] - 	                    (13, 3.597033281127612),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (14, 4.845926926533381),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (15, 6.487629123528799),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (16, 8.603293164571125),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (17, 11.264302984873455),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (18, 14.538702563444774),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (19, 18.534544261296592),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                    (20, 23.360640382766725)]}
[2025-02-10 15:32:03,071][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:32:03,071][flwr][INFO] - 	{'clients_accuracies': [(1, 0.11770833333333333),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (2, 0.18749999999999997),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (3, 0.265625),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (4, 0.26145833333333335),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (5, 0.3395833333333333),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (6, 0.45937500000000003),
[2025-02-10 15:32:03,071][flwr][INFO] - 	                        (7, 0.4958333333333333),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (8, 0.5041666666666667),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (9, 0.5197916666666667),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (10, 0.5291666666666666),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (11, 0.528125),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (12, 0.5145833333333333),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (13, 0.5052083333333333),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (14, 0.49687500000000007),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (15, 0.490625),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (16, 0.47395833333333337),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (17, 0.4614583333333333),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (18, 0.45208333333333334),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (19, 0.4364583333333334),
[2025-02-10 15:32:03,072][flwr][INFO] - 	                        (20, 0.4177083333333333)],
[2025-02-10 15:32:03,072][flwr][INFO] - 	 'clients_losses': [(1, 0.0766977918644746),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (2, 0.07641112556060156),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (3, 0.07554939364393552),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (4, 0.07334720542033514),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (5, 0.0690167294194301),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (6, 0.061200745527942975),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (7, 0.05209258235991002),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (8, 0.04823511689901352),
[2025-02-10 15:32:03,073][flwr][INFO] - 	                    (9, 0.05190100185573101),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (10, 0.06413151832918326),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (11, 0.08425165874262651),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (12, 0.11429199439783891),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (13, 0.1556917409102122),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (14, 0.2098839377363523),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (15, 0.27981051007906593),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (16, 0.3675472925106684),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (17, 0.47633898456891377),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (18, 0.6080653558174769),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (19, 0.7676865657170613),
[2025-02-10 15:32:03,074][flwr][INFO] - 	                    (20, 0.9610905567804973)]}
[2025-02-10 15:32:03,074][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:32:03,074][flwr][INFO] - 	{'accuracy': [(0, 0.1135),
[2025-02-10 15:32:03,074][flwr][INFO] - 	              (1, 0.1136),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (2, 0.1704),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (3, 0.2571),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (4, 0.2668),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (5, 0.3311),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (6, 0.4706),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (7, 0.512),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (8, 0.5226),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (9, 0.5359),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (10, 0.5382),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (11, 0.5393),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (12, 0.5288),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (13, 0.5156),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (14, 0.5049),
[2025-02-10 15:32:03,075][flwr][INFO] - 	              (15, 0.4903),
[2025-02-10 15:32:03,076][flwr][INFO] - 	              (16, 0.4762),
[2025-02-10 15:32:03,076][flwr][INFO] - 	              (17, 0.4617),
[2025-02-10 15:32:03,076][flwr][INFO] - 	              (18, 0.4505),
[2025-02-10 15:32:03,076][flwr][INFO] - 	              (19, 0.4412),
[2025-02-10 15:32:03,076][flwr][INFO] - 	              (20, 0.429)]}
[2025-02-10 15:32:03,076][flwr][INFO] - 
