[2025-02-10 15:32:26,341][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:32:26,342][flwr][INFO] - 
[2025-02-10 15:32:26,343][flwr][INFO] - [INIT]
[2025-02-10 15:32:26,343][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:32:26,344][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:32:32,370][flwr][INFO] - initial parameters (loss, other metrics): 0.07214015040397644, {'accuracy': 0.0988}
[2025-02-10 15:32:32,370][flwr][INFO] - 
[2025-02-10 15:32:32,371][flwr][INFO] - [ROUND 1]
[2025-02-10 15:32:32,371][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:32:39,251][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:39,250][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:39,268][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:39,269][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:39,380][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:39,380][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:45,304][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:32:50,241][flwr][INFO] - fit progress: (1, 0.07185981488227844, {'accuracy': 0.1343}, 17.87082307500532)
[2025-02-10 15:32:50,245][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:32:50,285][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:50,286][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:50,287][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:50,367][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:50,367][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:50,368][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:53,655][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:32:53,657][flwr][INFO] - 
[2025-02-10 15:32:53,657][flwr][INFO] - [ROUND 2]
[2025-02-10 15:32:53,658][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:32:53,773][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:53,773][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:53,774][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:32:53,774][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:32:53,775][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:53,776][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:32:56,271][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:00,225][flwr][INFO] - fit progress: (2, 0.0713579950094223, {'accuracy': 0.1372}, 27.854080429999158)
[2025-02-10 15:33:00,225][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:00,286][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:00,286][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:00,286][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:00,287][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:00,287][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:00,288][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:02,933][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:02,933][flwr][INFO] - 
[2025-02-10 15:33:02,933][flwr][INFO] - [ROUND 3]
[2025-02-10 15:33:02,933][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:02,990][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:02,990][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:02,991][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:02,991][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:02,991][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:02,993][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:05,540][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:09,736][flwr][INFO] - fit progress: (3, 0.07022321982383728, {'accuracy': 0.2983}, 37.3653778769949)
[2025-02-10 15:33:09,736][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:09,803][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:09,803][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:09,804][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:09,805][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:09,805][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:09,805][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:12,743][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:12,744][flwr][INFO] - 
[2025-02-10 15:33:12,744][flwr][INFO] - [ROUND 4]
[2025-02-10 15:33:12,744][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:12,809][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:12,810][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:12,811][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:12,812][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:12,811][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:12,813][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:15,550][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:19,687][flwr][INFO] - fit progress: (4, 0.06766726772785187, {'accuracy': 0.3462}, 47.316377501992974)
[2025-02-10 15:33:19,687][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:19,721][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:19,722][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:19,724][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:19,723][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:19,725][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:19,725][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:22,596][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:22,596][flwr][INFO] - 
[2025-02-10 15:33:22,596][flwr][INFO] - [ROUND 5]
[2025-02-10 15:33:22,596][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:22,629][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:22,629][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:22,629][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:22,630][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:22,629][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:22,632][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:25,305][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:29,406][flwr][INFO] - fit progress: (5, 0.0627415785908699, {'accuracy': 0.3673}, 57.035210539994296)
[2025-02-10 15:33:29,406][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:29,536][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:29,537][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:29,537][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:29,537][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:29,538][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:29,538][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:31,913][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:31,914][flwr][INFO] - 
[2025-02-10 15:33:31,914][flwr][INFO] - [ROUND 6]
[2025-02-10 15:33:31,914][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:31,951][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:31,952][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:31,954][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:31,954][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:31,955][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:31,956][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:34,622][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:38,546][flwr][INFO] - fit progress: (6, 0.05497486779689789, {'accuracy': 0.4601}, 66.17566066600557)
[2025-02-10 15:33:38,546][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:38,656][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:38,656][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:38,657][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:38,657][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:38,658][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:38,658][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:41,553][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:41,554][flwr][INFO] - 
[2025-02-10 15:33:41,554][flwr][INFO] - [ROUND 7]
[2025-02-10 15:33:41,554][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:41,667][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:41,668][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:41,669][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:41,670][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:41,670][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:41,672][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:44,367][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:48,299][flwr][INFO] - fit progress: (7, 0.0469125697016716, {'accuracy': 0.514}, 75.9283718719962)
[2025-02-10 15:33:48,299][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:48,378][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:48,379][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:48,381][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:48,382][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:48,382][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:48,381][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:51,207][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:33:51,208][flwr][INFO] - 
[2025-02-10 15:33:51,208][flwr][INFO] - [ROUND 8]
[2025-02-10 15:33:51,208][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:51,286][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:51,287][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:51,287][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:51,288][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:51,288][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:51,289][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:54,015][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:33:58,135][flwr][INFO] - fit progress: (8, 0.04266517176032066, {'accuracy': 0.5455}, 85.76498788199387)
[2025-02-10 15:33:58,136][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:33:58,200][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:58,200][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:33:58,200][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:58,201][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:33:58,200][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:33:58,202][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:03,846][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:03,847][flwr][INFO] - 
[2025-02-10 15:34:03,847][flwr][INFO] - [ROUND 9]
[2025-02-10 15:34:03,847][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:03,907][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:03,910][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:03,910][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:03,910][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:03,911][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:03,912][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:06,555][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:10,726][flwr][INFO] - fit progress: (9, 0.04238185636997223, {'accuracy': 0.5762}, 98.35535271899425)
[2025-02-10 15:34:10,726][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:10,820][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:10,821][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:10,822][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:10,821][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:10,823][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:10,823][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:13,534][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:13,534][flwr][INFO] - 
[2025-02-10 15:34:13,534][flwr][INFO] - [ROUND 10]
[2025-02-10 15:34:13,534][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:13,627][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:13,628][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:13,629][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:13,628][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:13,630][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:13,630][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:16,242][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:20,317][flwr][INFO] - fit progress: (10, 0.04596986781358719, {'accuracy': 0.5911}, 107.9468564120034)
[2025-02-10 15:34:20,318][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:20,342][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:20,342][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:20,343][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:20,343][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:20,343][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:20,344][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:23,424][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:23,424][flwr][INFO] - 
[2025-02-10 15:34:23,425][flwr][INFO] - [ROUND 11]
[2025-02-10 15:34:23,425][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:23,443][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:23,444][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:23,445][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:23,447][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:23,448][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:23,449][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:26,232][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:30,169][flwr][INFO] - fit progress: (11, 0.054550852698087696, {'accuracy': 0.5937}, 117.79894277299172)
[2025-02-10 15:34:30,170][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:30,259][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:30,259][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:30,259][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:30,260][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:30,261][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:30,262][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:33,078][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:33,079][flwr][INFO] - 
[2025-02-10 15:34:33,079][flwr][INFO] - [ROUND 12]
[2025-02-10 15:34:33,079][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:33,165][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:33,165][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:33,166][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:33,166][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:33,167][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:33,168][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:35,785][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:39,974][flwr][INFO] - fit progress: (12, 0.06926672843694687, {'accuracy': 0.5868}, 127.60342276199663)
[2025-02-10 15:34:39,974][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:40,077][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:40,078][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:40,079][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:40,078][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:40,080][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:40,081][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:42,782][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:42,782][flwr][INFO] - 
[2025-02-10 15:34:42,782][flwr][INFO] - [ROUND 13]
[2025-02-10 15:34:42,782][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:42,885][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:42,885][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:42,887][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:42,887][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:42,888][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:42,888][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:45,489][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:49,281][flwr][INFO] - fit progress: (13, 0.09135719772577286, {'accuracy': 0.5722}, 136.9109535269963)
[2025-02-10 15:34:49,282][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:49,395][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:49,397][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:49,397][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:49,397][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:49,398][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:49,399][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:52,090][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:34:52,090][flwr][INFO] - 
[2025-02-10 15:34:52,091][flwr][INFO] - [ROUND 14]
[2025-02-10 15:34:52,091][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:52,200][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:52,202][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:52,203][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:52,204][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:52,204][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:52,205][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:54,801][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:34:58,589][flwr][INFO] - fit progress: (14, 0.12223613183498383, {'accuracy': 0.552}, 146.2188853849948)
[2025-02-10 15:34:58,590][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:34:58,613][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:58,613][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:58,614][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:34:58,616][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:34:58,616][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:34:58,617][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:00,998][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:00,998][flwr][INFO] - 
[2025-02-10 15:35:00,998][flwr][INFO] - [ROUND 15]
[2025-02-10 15:35:00,998][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:01,109][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:01,109][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:01,109][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:01,109][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:01,110][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:01,110][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:03,708][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:07,507][flwr][INFO] - fit progress: (15, 0.1635779154777527, {'accuracy': 0.532}, 155.13654023199342)
[2025-02-10 15:35:07,507][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:07,528][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:07,531][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:07,532][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:07,530][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:07,532][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:07,533][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:10,216][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:10,217][flwr][INFO] - 
[2025-02-10 15:35:10,217][flwr][INFO] - [ROUND 16]
[2025-02-10 15:35:10,217][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:10,235][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:10,236][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:10,237][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:10,237][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:10,238][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:10,239][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:12,824][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:16,607][flwr][INFO] - fit progress: (16, 0.21762611348629, {'accuracy': 0.5106}, 164.2367538739927)
[2025-02-10 15:35:16,607][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:16,646][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:16,647][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:16,647][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:16,649][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:16,650][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:16,650][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:19,217][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:19,217][flwr][INFO] - 
[2025-02-10 15:35:19,217][flwr][INFO] - [ROUND 17]
[2025-02-10 15:35:19,218][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:19,252][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:19,253][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:19,253][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:19,253][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:19,254][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:19,254][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:22,024][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:25,874][flwr][INFO] - fit progress: (17, 0.28584121084213254, {'accuracy': 0.4906}, 173.50354573699587)
[2025-02-10 15:35:25,874][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:25,963][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:25,963][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:25,964][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:25,964][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:25,964][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:25,965][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:28,783][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:28,784][flwr][INFO] - 
[2025-02-10 15:35:28,784][flwr][INFO] - [ROUND 18]
[2025-02-10 15:35:28,784][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:28,869][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:28,869][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:28,869][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:28,870][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:28,871][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:28,871][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:31,490][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:35,189][flwr][INFO] - fit progress: (18, 0.37088675165176394, {'accuracy': 0.4744}, 182.8188779530028)
[2025-02-10 15:35:35,190][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:35,278][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:35,278][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:35,278][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:35,280][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:35,282][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:35,282][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:38,296][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:38,297][flwr][INFO] - 
[2025-02-10 15:35:38,297][flwr][INFO] - [ROUND 19]
[2025-02-10 15:35:38,297][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:38,386][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:38,387][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:38,387][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:38,387][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:38,388][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:38,388][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:40,906][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:45,057][flwr][INFO] - fit progress: (19, 0.47531593308448794, {'accuracy': 0.4599}, 192.68694281099306)
[2025-02-10 15:35:45,058][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:45,101][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:45,101][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:45,102][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:45,102][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:45,102][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:45,103][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:47,865][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:47,865][flwr][INFO] - 
[2025-02-10 15:35:47,865][flwr][INFO] - [ROUND 20]
[2025-02-10 15:35:47,865][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:47,902][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:47,903][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:47,904][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:47,905][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:47,905][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:47,906][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:50,373][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:35:54,267][flwr][INFO] - fit progress: (20, 0.6016535260200501, {'accuracy': 0.4474}, 201.89696460399136)
[2025-02-10 15:35:54,268][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:35:54,313][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:54,314][flwr][ERROR] - Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:54,315][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:54,315][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:35:54,316][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:35:54,317][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 316fb3e9bca84d788c66d23b62de3db22222da193bf7768427058505) where the task (actor ID: 4cf7871b27ce590164dc21d001000000, name=ClientAppActor.__init__, pid=8900, memory used=0.42GB) was running was 7.18GB / 7.45GB (0.964057), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-0ff92b37ab9946bde6ee589a6fa8e35666ec827886d6d280f10189bc*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
773	0.52	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node /home/theo_ubuntu...
8896	0.46	ray::ClientAppActor.run
588	0.45	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
8891	0.45	ray::ClientAppActor.run
8901	0.44	ray::ClientAppActor.run
8899	0.43	ray::ClientAppActor.run
8898	0.42	ray::ClientAppActor.run
8897	0.42	ray::ClientAppActor.run
8051	0.42	python /home/theo_ubuntu/Diplomatic_incident/Differential_Privacy/Mnist_2/opacus_fl/main.py SEED=222...
8900	0.42	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:35:58,178][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:35:58,179][flwr][INFO] - 
[2025-02-10 15:35:58,179][flwr][INFO] - [SUMMARY]
[2025-02-10 15:35:58,179][flwr][INFO] - Run finished 20 round(s) in 205.81s
[2025-02-10 15:35:58,185][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:35:58,185][flwr][INFO] - 		round 1: 0.07649244517087936
[2025-02-10 15:35:58,185][flwr][INFO] - 		round 2: 0.07600577995181083
[2025-02-10 15:35:58,185][flwr][INFO] - 		round 3: 0.0748497448861599
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 4: 0.07217203378677367
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 5: 0.0671884937832753
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 6: 0.05915403092900912
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 7: 0.050963175110518934
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 8: 0.046924337496360144
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 9: 0.046933345248301825
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 10: 0.05119178971896569
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 11: 0.06121306971957287
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 12: 0.07776418812572956
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 13: 0.102126559180518
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 14: 0.13578188841541608
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 15: 0.181752385571599
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 16: 0.24123014857371647
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 17: 0.316448384275039
[2025-02-10 15:35:58,186][flwr][INFO] - 		round 18: 0.41020347476005553
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 19: 0.5252386644482613
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 20: 0.6655616223812103
[2025-02-10 15:35:58,187][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 0: 0.07214015040397644
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 1: 0.07185981488227844
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 2: 0.0713579950094223
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 3: 0.07022321982383728
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 4: 0.06766726772785187
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 5: 0.0627415785908699
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 6: 0.05497486779689789
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 7: 0.0469125697016716
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 8: 0.04266517176032066
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 9: 0.04238185636997223
[2025-02-10 15:35:58,187][flwr][INFO] - 		round 10: 0.04596986781358719
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 11: 0.054550852698087696
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 12: 0.06926672843694687
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 13: 0.09135719772577286
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 14: 0.12223613183498383
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 15: 0.1635779154777527
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 16: 0.21762611348629
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 17: 0.28584121084213254
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 18: 0.37088675165176394
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 19: 0.47531593308448794
[2025-02-10 15:35:58,188][flwr][INFO] - 		round 20: 0.6016535260200501
[2025-02-10 15:35:58,188][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:35:58,188][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867437),
[2025-02-10 15:35:58,188][flwr][INFO] - 	                      (2, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (3, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (4, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (5, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (6, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (7, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (8, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (9, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (10, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (11, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (12, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (13, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (14, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (15, 0.6813471387867437),
[2025-02-10 15:35:58,189][flwr][INFO] - 	                      (16, 0.6813471387867437),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                      (17, 0.6813471387867437),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                      (18, 0.6813471387867437),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                      (19, 0.6813471387867437),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                      (20, 0.6813471387867437)],
[2025-02-10 15:35:58,190][flwr][INFO] - 	 'clients_losses': [(1, 2.3004417300224302),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (2, 2.2910397688547772),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (3, 2.2693196892738343),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (4, 2.220010034243266),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (5, 2.1157701929410297),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (6, 1.9328418125708897),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (7, 1.6903149257103602),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (8, 1.5089120229085287),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (9, 1.4605834563573201),
[2025-02-10 15:35:58,190][flwr][INFO] - 	                    (10, 1.540634998679161),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (11, 1.767535384496053),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (12, 2.1775816390911737),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (13, 2.8021092345317204),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (14, 3.6821033358573914),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (15, 4.954717723528544),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (16, 6.4421759724617),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (17, 8.457021717230479),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (18, 10.963895070552827),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (19, 14.070765022436778),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                    (20, 17.816057499249776)]}
[2025-02-10 15:35:58,191][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:35:58,191][flwr][INFO] - 	{'clients_accuracies': [(1, 0.159375),
[2025-02-10 15:35:58,191][flwr][INFO] - 	                        (2, 0.13645833333333335),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (3, 0.2875),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (4, 0.3510416666666666),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (5, 0.36979166666666663),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (6, 0.4552083333333334),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (7, 0.5104166666666666),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (8, 0.5364583333333333),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (9, 0.5666666666666667),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (10, 0.5864583333333333),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (11, 0.5927083333333333),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (12, 0.5822916666666667),
[2025-02-10 15:35:58,192][flwr][INFO] - 	                        (13, 0.5666666666666667),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (14, 0.5479166666666667),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (15, 0.5260416666666666),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (16, 0.5145833333333334),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (17, 0.48437500000000006),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (18, 0.4666666666666667),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (19, 0.4614583333333333),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                        (20, 0.4510416666666666)],
[2025-02-10 15:35:58,193][flwr][INFO] - 	 'clients_losses': [(1, 0.07649244517087936),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (2, 0.07600577995181083),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (3, 0.0748497448861599),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (4, 0.07217203378677367),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (5, 0.0671884937832753),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (6, 0.05915403092900912),
[2025-02-10 15:35:58,193][flwr][INFO] - 	                    (7, 0.050963175110518934),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (8, 0.046924337496360144),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (9, 0.046933345248301825),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (10, 0.05119178971896569),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (11, 0.06121306971957287),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (12, 0.07776418812572956),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (13, 0.102126559180518),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (14, 0.13578188841541608),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (15, 0.181752385571599),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (16, 0.24123014857371647),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (17, 0.316448384275039),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (18, 0.41020347476005553),
[2025-02-10 15:35:58,194][flwr][INFO] - 	                    (19, 0.5252386644482613),
[2025-02-10 15:35:58,195][flwr][INFO] - 	                    (20, 0.6655616223812103)]}
[2025-02-10 15:35:58,195][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:35:58,195][flwr][INFO] - 	{'accuracy': [(0, 0.0988),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (1, 0.1343),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (2, 0.1372),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (3, 0.2983),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (4, 0.3462),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (5, 0.3673),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (6, 0.4601),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (7, 0.514),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (8, 0.5455),
[2025-02-10 15:35:58,195][flwr][INFO] - 	              (9, 0.5762),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (10, 0.5911),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (11, 0.5937),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (12, 0.5868),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (13, 0.5722),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (14, 0.552),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (15, 0.532),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (16, 0.5106),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (17, 0.4906),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (18, 0.4744),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (19, 0.4599),
[2025-02-10 15:35:58,196][flwr][INFO] - 	              (20, 0.4474)]}
[2025-02-10 15:35:58,196][flwr][INFO] - 
