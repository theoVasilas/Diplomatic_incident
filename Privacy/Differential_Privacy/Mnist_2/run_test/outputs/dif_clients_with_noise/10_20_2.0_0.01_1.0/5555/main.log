[2025-02-10 15:44:02,784][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:44:02,786][flwr][INFO] - 
[2025-02-10 15:44:02,787][flwr][INFO] - [INIT]
[2025-02-10 15:44:02,787][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:44:02,788][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:44:08,874][flwr][INFO] - initial parameters (loss, other metrics): 0.07218380236625671, {'accuracy': 0.0859}
[2025-02-10 15:44:08,875][flwr][INFO] - 
[2025-02-10 15:44:08,875][flwr][INFO] - [ROUND 1]
[2025-02-10 15:44:08,875][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:14,664][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:14,682][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:14,699][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:14,704][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:14,792][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:14,794][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:20,013][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:44:25,073][flwr][INFO] - fit progress: (1, 0.07165103061199188, {'accuracy': 0.1708}, 16.198468782997224)
[2025-02-10 15:44:25,077][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:25,191][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:25,192][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:25,192][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:25,192][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:25,193][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:25,194][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:27,686][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:44:27,687][flwr][INFO] - 
[2025-02-10 15:44:27,687][flwr][INFO] - [ROUND 2]
[2025-02-10 15:44:27,688][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:27,804][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:27,805][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:27,806][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:27,805][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:27,807][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:27,808][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:30,700][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:44:34,679][flwr][INFO] - fit progress: (2, 0.07097502670288086, {'accuracy': 0.1845}, 25.80440778400225)
[2025-02-10 15:44:34,679][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:34,716][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:34,716][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:34,718][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:34,720][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:34,720][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:34,721][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:37,087][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:44:37,088][flwr][INFO] - 
[2025-02-10 15:44:37,088][flwr][INFO] - [ROUND 3]
[2025-02-10 15:44:37,088][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:37,122][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:37,122][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:37,122][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:37,123][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:37,124][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:37,125][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:39,695][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:44:43,618][flwr][INFO] - fit progress: (3, 0.06953684067726135, {'accuracy': 0.2952}, 34.74395449500298)
[2025-02-10 15:44:43,619][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:43,731][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:43,731][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:43,731][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:43,732][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:43,731][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:43,733][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:46,330][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:44:46,330][flwr][INFO] - 
[2025-02-10 15:44:46,330][flwr][INFO] - [ROUND 4]
[2025-02-10 15:44:46,331][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:46,439][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:46,439][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:46,441][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:46,441][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:46,442][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:46,442][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:49,042][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:44:52,991][flwr][INFO] - fit progress: (4, 0.06659034526348113, {'accuracy': 0.33}, 44.11676932699629)
[2025-02-10 15:44:52,992][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:53,051][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:53,052][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:53,053][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:53,053][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:53,054][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:53,055][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:55,597][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:44:55,597][flwr][INFO] - 
[2025-02-10 15:44:55,598][flwr][INFO] - [ROUND 5]
[2025-02-10 15:44:55,598][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:44:55,657][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:55,657][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:44:55,658][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:55,659][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:55,658][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:44:55,659][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:44:58,205][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:02,171][flwr][INFO] - fit progress: (5, 0.06133231691122055, {'accuracy': 0.4321}, 53.29639579700597)
[2025-02-10 15:45:02,171][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:02,269][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:02,270][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:02,271][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:02,271][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:02,270][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:02,273][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:05,303][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:05,303][flwr][INFO] - 
[2025-02-10 15:45:05,304][flwr][INFO] - [ROUND 6]
[2025-02-10 15:45:05,304][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:05,376][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:05,377][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:05,377][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:05,378][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:05,378][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:05,379][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:07,913][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:11,787][flwr][INFO] - fit progress: (6, 0.0533206953048706, {'accuracy': 0.49}, 62.912828290005564)
[2025-02-10 15:45:11,788][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:11,888][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:11,889][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:11,890][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:11,890][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:11,891][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:11,891][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:14,594][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:14,594][flwr][INFO] - 
[2025-02-10 15:45:14,594][flwr][INFO] - [ROUND 7]
[2025-02-10 15:45:14,594][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:14,693][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:14,694][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:14,695][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:14,696][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:14,696][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:14,697][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:17,802][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:21,625][flwr][INFO] - fit progress: (7, 0.045461046588420866, {'accuracy': 0.5208}, 72.75017916499928)
[2025-02-10 15:45:21,625][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:21,706][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:21,708][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:21,708][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:21,709][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:21,709][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:21,710][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:24,632][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:24,632][flwr][INFO] - 
[2025-02-10 15:45:24,632][flwr][INFO] - [ROUND 8]
[2025-02-10 15:45:24,632][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:24,714][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:24,714][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:24,715][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:24,716][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:24,716][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:24,717][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:27,343][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:31,610][flwr][INFO] - fit progress: (8, 0.041274250334501265, {'accuracy': 0.5514}, 82.73529246500402)
[2025-02-10 15:45:31,610][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:31,721][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:31,721][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:31,721][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:31,722][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:31,723][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:31,723][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:34,624][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:34,625][flwr][INFO] - 
[2025-02-10 15:45:34,625][flwr][INFO] - [ROUND 9]
[2025-02-10 15:45:34,625][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:34,730][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:34,733][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:34,733][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:34,734][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:34,734][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:34,735][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:37,233][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:41,171][flwr][INFO] - fit progress: (9, 0.04179623057842255, {'accuracy': 0.5698}, 92.29650615900755)
[2025-02-10 15:45:41,171][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:41,242][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:41,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:41,244][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:41,242][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:41,244][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:41,245][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:44,480][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:44,480][flwr][INFO] - 
[2025-02-10 15:45:44,480][flwr][INFO] - [ROUND 10]
[2025-02-10 15:45:44,480][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:44,548][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:44,549][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:44,550][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:44,551][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:44,551][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:44,552][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:47,089][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:45:51,075][flwr][INFO] - fit progress: (10, 0.047283399665355684, {'accuracy': 0.5804}, 102.20036342099775)
[2025-02-10 15:45:51,075][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:51,164][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:51,164][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:51,165][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:51,165][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:51,166][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:51,166][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:53,982][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:45:53,983][flwr][INFO] - 
[2025-02-10 15:45:53,983][flwr][INFO] - [ROUND 11]
[2025-02-10 15:45:53,983][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:45:54,070][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:54,071][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:45:54,072][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:54,073][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:45:54,073][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:54,074][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:45:56,591][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:00,267][flwr][INFO] - fit progress: (11, 0.05791104789972305, {'accuracy': 0.5845}, 111.39255570199748)
[2025-02-10 15:46:00,267][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:00,381][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:00,381][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:00,381][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:00,382][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:00,383][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:00,383][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:03,480][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:03,480][flwr][INFO] - 
[2025-02-10 15:46:03,480][flwr][INFO] - [ROUND 12]
[2025-02-10 15:46:03,480][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:03,588][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:03,588][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:03,589][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:03,589][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:03,590][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:03,591][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:06,189][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:09,796][flwr][INFO] - fit progress: (12, 0.07438862280845641, {'accuracy': 0.576}, 120.92143855300674)
[2025-02-10 15:46:09,796][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:09,899][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:09,902][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:09,902][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:09,902][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:09,903][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:09,903][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:12,806][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:12,807][flwr][INFO] - 
[2025-02-10 15:46:12,807][flwr][INFO] - [ROUND 13]
[2025-02-10 15:46:12,807][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:12,906][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:12,907][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:12,908][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:12,909][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:12,909][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:12,910][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:15,416][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:19,334][flwr][INFO] - fit progress: (13, 0.09800116037130356, {'accuracy': 0.5628}, 130.45957327300857)
[2025-02-10 15:46:19,334][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:19,418][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:19,418][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:19,419][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:19,420][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:19,420][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:19,421][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:22,142][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:22,142][flwr][INFO] - 
[2025-02-10 15:46:22,143][flwr][INFO] - [ROUND 14]
[2025-02-10 15:46:22,143][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:22,224][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:22,225][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:22,225][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:22,226][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:22,227][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:22,227][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:25,650][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:29,887][flwr][INFO] - fit progress: (14, 0.1308773953437805, {'accuracy': 0.5417}, 141.01241472299444)
[2025-02-10 15:46:29,887][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:29,937][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:29,938][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:29,939][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:29,940][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:29,940][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:29,941][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:32,495][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:32,495][flwr][INFO] - 
[2025-02-10 15:46:32,495][flwr][INFO] - [ROUND 15]
[2025-02-10 15:46:32,495][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:32,544][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:32,544][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:32,544][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:32,545][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:32,546][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:32,546][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:35,106][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:38,781][flwr][INFO] - fit progress: (15, 0.17321500878334045, {'accuracy': 0.5284}, 149.9059616710001)
[2025-02-10 15:46:38,781][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:38,855][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:38,855][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:38,856][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:38,857][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:38,857][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:38,858][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:41,390][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:41,391][flwr][INFO] - 
[2025-02-10 15:46:41,391][flwr][INFO] - [ROUND 16]
[2025-02-10 15:46:41,391][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:41,461][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:41,461][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:41,462][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:41,462][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:41,461][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:41,464][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:44,000][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:47,886][flwr][INFO] - fit progress: (16, 0.22738314123153686, {'accuracy': 0.5167}, 159.01153894800518)
[2025-02-10 15:46:47,886][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:47,973][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:47,974][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:47,975][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:47,975][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:47,975][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:47,976][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:50,994][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:46:50,994][flwr][INFO] - 
[2025-02-10 15:46:50,994][flwr][INFO] - [ROUND 17]
[2025-02-10 15:46:50,994][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:51,079][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:51,080][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:51,081][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:51,081][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:51,082][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:51,082][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:54,202][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:46:57,926][flwr][INFO] - fit progress: (17, 0.29514394249916076, {'accuracy': 0.5012}, 169.05114240999683)
[2025-02-10 15:46:57,926][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:46:57,989][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:57,990][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:57,991][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:46:57,992][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:46:57,993][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:46:57,994][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:00,734][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:47:00,734][flwr][INFO] - 
[2025-02-10 15:47:00,734][flwr][INFO] - [ROUND 18]
[2025-02-10 15:47:00,734][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:00,797][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:00,797][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:00,798][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:00,799][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:00,799][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:00,800][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:04,142][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:47:07,892][flwr][INFO] - fit progress: (18, 0.37915784697532656, {'accuracy': 0.4915}, 179.0178678749944)
[2025-02-10 15:47:07,893][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:08,002][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:08,003][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:08,003][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:08,003][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:08,004][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:08,004][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:10,499][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:47:10,499][flwr][INFO] - 
[2025-02-10 15:47:10,499][flwr][INFO] - [ROUND 19]
[2025-02-10 15:47:10,499][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:10,613][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:10,613][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:10,613][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:10,614][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:10,615][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:10,615][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:13,112][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:47:17,008][flwr][INFO] - fit progress: (19, 0.4824638256072998, {'accuracy': 0.4821}, 188.13314203299524)
[2025-02-10 15:47:17,008][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:17,117][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:17,117][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:17,118][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:17,118][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:17,119][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:17,119][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:19,715][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:47:19,716][flwr][INFO] - 
[2025-02-10 15:47:19,716][flwr][INFO] - [ROUND 20]
[2025-02-10 15:47:19,716][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:19,829][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:19,830][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:19,831][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:19,831][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:19,832][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:19,832][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:22,428][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:47:26,153][flwr][INFO] - fit progress: (20, 0.6069127386093139, {'accuracy': 0.4719}, 197.27831518399762)
[2025-02-10 15:47:26,153][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:47:26,243][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:26,243][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:47:26,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:26,244][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 37a3daa417f403503372739901000000, name=ClientAppActor.__init__, pid=27498, memory used=0.50GB) was running was 7.33GB / 7.45GB (0.984396), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-f8bbe3c5cdaddb9c6dc540b9c3bc40284a3e8d2549ba96b416113835*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27498	0.50	ray::ClientAppActor.run
27493	0.50	ray::ClientAppActor.run
27494	0.50	ray::ClientAppActor.run
27491	0.50	ray::ClientAppActor.run
27490	0.50	ray::ClientAppActor.run
27497	0.50	ray::ClientAppActor.run
27496	0.50	ray::ClientAppActor.run
27495	0.50	ray::ClientAppActor.run
27484	0.50	ray::ClientAppActor.run
27492	0.49	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:26,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:47:26,245][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 14273e71935fa1a79ae15b52aad8bd6e5836c417c65d39ac9911da25) where the task (actor ID: 47b625b4747e23aab35a53a701000000, name=ClientAppActor.__init__, pid=27493, memory used=0.58GB) was running was 7.39GB / 7.45GB (0.991804), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-ea545d8590908f174355834c5873ecbb408d374bdec49582da71d4b3*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
27493	0.58	ray::ClientAppActor.run
27490	0.58	ray::ClientAppActor.run
27494	0.58	ray::ClientAppActor.run
27495	0.58	ray::ClientAppActor.run
27496	0.58	ray::ClientAppActor.run
27497	0.58	ray::ClientAppActor.run
27491	0.58	ray::ClientAppActor.run
27484	0.58	ray::ClientAppActor.run
27492	0.57	ray::ClientAppActor.run
588	0.30	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:47:29,059][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:47:29,063][flwr][INFO] - 
[2025-02-10 15:47:29,063][flwr][INFO] - [SUMMARY]
[2025-02-10 15:47:29,063][flwr][INFO] - Run finished 20 round(s) in 200.18s
[2025-02-10 15:47:29,070][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 1: 0.07637885188062984
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 2: 0.07563615764180819
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 3: 0.07417216946681339
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 4: 0.07115975022315979
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 5: 0.06579013504087924
[2025-02-10 15:47:29,070][flwr][INFO] - 		round 6: 0.057666626075903574
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 7: 0.04980880493919054
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 8: 0.045898748189210886
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 9: 0.04726686297605435
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 10: 0.054340231232345104
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 11: 0.06712730526924134
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 12: 0.08670551429192226
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 13: 0.11357877813279628
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 14: 0.14995354289809865
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 15: 0.19680079668760297
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 16: 0.25708913306395215
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 17: 0.33238875965277354
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 18: 0.41623851209878926
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 19: 0.5024186511834462
[2025-02-10 15:47:29,071][flwr][INFO] - 		round 20: 0.6824146330356599
[2025-02-10 15:47:29,072][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 0: 0.07218380236625671
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 1: 0.07165103061199188
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 2: 0.07097502670288086
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 3: 0.06953684067726135
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 4: 0.06659034526348113
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 5: 0.06133231691122055
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 6: 0.0533206953048706
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 7: 0.045461046588420866
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 8: 0.041274250334501265
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 9: 0.04179623057842255
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 10: 0.047283399665355684
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 11: 0.05791104789972305
[2025-02-10 15:47:29,072][flwr][INFO] - 		round 12: 0.07438862280845641
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 13: 0.09800116037130356
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 14: 0.1308773953437805
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 15: 0.17321500878334045
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 16: 0.22738314123153686
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 17: 0.29514394249916076
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 18: 0.37915784697532656
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 19: 0.4824638256072998
[2025-02-10 15:47:29,073][flwr][INFO] - 		round 20: 0.6069127386093139
[2025-02-10 15:47:29,073][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:47:29,073][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867437),
[2025-02-10 15:47:29,073][flwr][INFO] - 	                      (2, 0.6813471387867437),
[2025-02-10 15:47:29,073][flwr][INFO] - 	                      (3, 0.6813471387867437),
[2025-02-10 15:47:29,073][flwr][INFO] - 	                      (4, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (5, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (6, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (7, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (8, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (9, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (10, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (11, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (12, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (13, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (14, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (15, 0.6813471387867437),
[2025-02-10 15:47:29,074][flwr][INFO] - 	                      (16, 0.6813471387867437),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                      (17, 0.6813471387867437),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                      (18, 0.6813471387867437),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                      (19, 0.6813471387867437),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                      (20, 0.6813471387867437)],
[2025-02-10 15:47:29,075][flwr][INFO] - 	 'clients_losses': [(1, 2.3013921717802686),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (2, 2.2844699760278067),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (3, 2.2566758672396343),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (4, 2.1965616683165234),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (5, 2.08153341114521),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (6, 1.8870653520027796),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (7, 1.634276600678762),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (8, 1.4357643187046052),
[2025-02-10 15:47:29,075][flwr][INFO] - 	                    (9, 1.3610148986180624),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (10, 1.450873584051927),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (11, 1.7087586810191473),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (12, 2.1374686787525814),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (13, 2.761233016848564),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (14, 3.625865238904953),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (15, 4.7726614813009895),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (16, 6.267981612682343),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (17, 8.174034065008163),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (18, 10.54754299322764),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (19, 13.472986857096354),
[2025-02-10 15:47:29,076][flwr][INFO] - 	                    (20, 16.995979313055674)]}
[2025-02-10 15:47:29,076][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:47:29,076][flwr][INFO] - 	{'clients_accuracies': [(1, 0.159375),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (2, 0.18958333333333335),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (3, 0.278125),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (4, 0.309375),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (5, 0.4208333333333334),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (6, 0.48020833333333335),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (7, 0.5104166666666666),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (8, 0.5395833333333333),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (9, 0.5593750000000001),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (10, 0.58125),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (11, 0.5864583333333333),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (12, 0.5802083333333333),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (13, 0.5645833333333333),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (14, 0.5510416666666667),
[2025-02-10 15:47:29,077][flwr][INFO] - 	                        (15, 0.5302083333333333),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                        (16, 0.5104166666666666),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                        (17, 0.49375),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                        (18, 0.4864583333333333),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                        (19, 0.47604166666666664),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                        (20, 0.4666666666666667)],
[2025-02-10 15:47:29,078][flwr][INFO] - 	 'clients_losses': [(1, 0.07637885188062984),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                    (2, 0.07563615764180819),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                    (3, 0.07417216946681339),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                    (4, 0.07115975022315979),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                    (5, 0.06579013504087924),
[2025-02-10 15:47:29,078][flwr][INFO] - 	                    (6, 0.057666626075903574),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (7, 0.04980880493919054),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (8, 0.045898748189210886),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (9, 0.04726686297605435),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (10, 0.054340231232345104),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (11, 0.06712730526924134),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (12, 0.08670551429192226),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (13, 0.11357877813279628),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (14, 0.14995354289809865),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (15, 0.19680079668760297),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (16, 0.25708913306395215),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (17, 0.33238875965277354),
[2025-02-10 15:47:29,079][flwr][INFO] - 	                    (18, 0.41623851209878926),
[2025-02-10 15:47:29,080][flwr][INFO] - 	                    (19, 0.5024186511834462),
[2025-02-10 15:47:29,080][flwr][INFO] - 	                    (20, 0.6824146330356599)]}
[2025-02-10 15:47:29,080][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:47:29,080][flwr][INFO] - 	{'accuracy': [(0, 0.0859),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (1, 0.1708),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (2, 0.1845),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (3, 0.2952),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (4, 0.33),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (5, 0.4321),
[2025-02-10 15:47:29,080][flwr][INFO] - 	              (6, 0.49),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (7, 0.5208),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (8, 0.5514),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (9, 0.5698),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (10, 0.5804),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (11, 0.5845),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (12, 0.576),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (13, 0.5628),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (14, 0.5417),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (15, 0.5284),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (16, 0.5167),
[2025-02-10 15:47:29,081][flwr][INFO] - 	              (17, 0.5012),
[2025-02-10 15:47:29,082][flwr][INFO] - 	              (18, 0.4915),
[2025-02-10 15:47:29,082][flwr][INFO] - 	              (19, 0.4821),
[2025-02-10 15:47:29,082][flwr][INFO] - 	              (20, 0.4719)]}
[2025-02-10 15:47:29,082][flwr][INFO] - 
