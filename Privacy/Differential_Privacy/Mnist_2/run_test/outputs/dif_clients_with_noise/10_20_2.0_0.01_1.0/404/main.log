[2025-02-10 15:40:10,605][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:40:10,606][flwr][INFO] - 
[2025-02-10 15:40:10,607][flwr][INFO] - [INIT]
[2025-02-10 15:40:10,607][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:40:10,608][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:40:16,670][flwr][INFO] - initial parameters (loss, other metrics): 0.0721090440750122, {'accuracy': 0.0788}
[2025-02-10 15:40:16,670][flwr][INFO] - 
[2025-02-10 15:40:16,670][flwr][INFO] - [ROUND 1]
[2025-02-10 15:40:16,671][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:22,744][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:22,767][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:22,893][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:27,418][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:40:32,220][flwr][INFO] - fit progress: (1, 0.07171215100288392, {'accuracy': 0.0981}, 15.549893436997081)
[2025-02-10 15:40:32,224][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:32,281][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:32,282][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:32,285][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:35,540][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:40:35,542][flwr][INFO] - 
[2025-02-10 15:40:35,542][flwr][INFO] - [ROUND 2]
[2025-02-10 15:40:35,543][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:35,657][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:35,658][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:35,659][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:38,354][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:40:42,407][flwr][INFO] - fit progress: (2, 0.07109681708812714, {'accuracy': 0.1262}, 25.73716661099752)
[2025-02-10 15:40:42,408][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:42,470][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:42,470][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:42,471][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:45,314][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:40:45,314][flwr][INFO] - 
[2025-02-10 15:40:45,315][flwr][INFO] - [ROUND 3]
[2025-02-10 15:40:45,315][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:45,376][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:45,377][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:45,378][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:48,022][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:40:52,011][flwr][INFO] - fit progress: (3, 0.06966230854988098, {'accuracy': 0.3167}, 35.34065725999244)
[2025-02-10 15:40:52,011][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:52,088][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:52,088][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:52,089][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:55,318][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:40:55,319][flwr][INFO] - 
[2025-02-10 15:40:55,319][flwr][INFO] - [ROUND 4]
[2025-02-10 15:40:55,319][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:40:55,397][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:40:55,397][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:40:55,398][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:40:58,027][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:02,122][flwr][INFO] - fit progress: (4, 0.06643684768676758, {'accuracy': 0.3638}, 45.45139415899757)
[2025-02-10 15:41:02,122][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:02,209][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:02,209][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:02,210][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:05,028][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:05,029][flwr][INFO] - 
[2025-02-10 15:41:05,029][flwr][INFO] - [ROUND 5]
[2025-02-10 15:41:05,029][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:05,115][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:05,115][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:05,116][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:08,038][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:12,207][flwr][INFO] - fit progress: (5, 0.06041395205259323, {'accuracy': 0.4264}, 55.53686501199263)
[2025-02-10 15:41:12,207][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:12,342][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:12,342][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:12,344][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:15,236][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:15,237][flwr][INFO] - 
[2025-02-10 15:41:15,237][flwr][INFO] - [ROUND 6]
[2025-02-10 15:41:15,237][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:15,349][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:15,350][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:15,350][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:18,848][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:22,740][flwr][INFO] - fit progress: (6, 0.05224622600078583, {'accuracy': 0.4974}, 66.06931486100075)
[2025-02-10 15:41:22,740][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:22,761][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:22,761][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:22,762][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:25,547][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:25,548][flwr][INFO] - 
[2025-02-10 15:41:25,548][flwr][INFO] - [ROUND 7]
[2025-02-10 15:41:25,548][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:25,567][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:25,567][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:25,568][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:28,155][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:32,135][flwr][INFO] - fit progress: (7, 0.045226541846990584, {'accuracy': 0.5314}, 75.46433826700377)
[2025-02-10 15:41:32,135][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:32,177][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:32,178][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:32,179][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:35,041][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:35,041][flwr][INFO] - 
[2025-02-10 15:41:35,041][flwr][INFO] - [ROUND 8]
[2025-02-10 15:41:35,042][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:35,085][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:35,085][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:35,086][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:37,651][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:41,558][flwr][INFO] - fit progress: (8, 0.042820585495233536, {'accuracy': 0.5517}, 84.88746232599078)
[2025-02-10 15:41:41,558][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:41,597][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:41,598][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:41,598][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:44,464][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:44,465][flwr][INFO] - 
[2025-02-10 15:41:44,465][flwr][INFO] - [ROUND 9]
[2025-02-10 15:41:44,465][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:44,504][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:44,504][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:44,505][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:49,076][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:41:52,892][flwr][INFO] - fit progress: (9, 0.04616017283797264, {'accuracy': 0.5643}, 96.22206612500304)
[2025-02-10 15:41:52,893][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:52,919][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:52,920][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:52,921][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:55,699][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:41:55,699][flwr][INFO] - 
[2025-02-10 15:41:55,699][flwr][INFO] - [ROUND 10]
[2025-02-10 15:41:55,700][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:41:55,725][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:41:55,726][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:41:55,727][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:41:58,318][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:02,308][flwr][INFO] - fit progress: (10, 0.053879778122901914, {'accuracy': 0.5777}, 105.63745508399734)
[2025-02-10 15:42:02,308][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:02,336][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:02,337][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:02,338][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:05,317][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:05,318][flwr][INFO] - 
[2025-02-10 15:42:05,318][flwr][INFO] - [ROUND 11]
[2025-02-10 15:42:05,318][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:05,342][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:05,343][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:05,344][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:07,926][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:11,645][flwr][INFO] - fit progress: (11, 0.06638471059203148, {'accuracy': 0.5813}, 114.97454705099517)
[2025-02-10 15:42:11,645][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:11,753][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:11,754][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:11,756][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:14,755][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:14,755][flwr][INFO] - 
[2025-02-10 15:42:14,755][flwr][INFO] - [ROUND 12]
[2025-02-10 15:42:14,755][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:14,862][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:14,862][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:14,863][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:17,462][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:21,309][flwr][INFO] - fit progress: (12, 0.08535963147878647, {'accuracy': 0.5774}, 124.63877400300407)
[2025-02-10 15:42:21,309][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:21,374][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:21,375][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:21,375][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:23,817][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:23,817][flwr][INFO] - 
[2025-02-10 15:42:23,817][flwr][INFO] - [ROUND 13]
[2025-02-10 15:42:23,818][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:23,880][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:23,880][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:23,881][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:26,530][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:30,244][flwr][INFO] - fit progress: (13, 0.11187102482318878, {'accuracy': 0.5651}, 133.5736777150014)
[2025-02-10 15:42:30,244][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:30,291][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:30,291][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:30,292][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:33,252][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:33,252][flwr][INFO] - 
[2025-02-10 15:42:33,252][flwr][INFO] - [ROUND 14]
[2025-02-10 15:42:33,252][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:33,297][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:33,298][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:33,298][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:35,859][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:39,575][flwr][INFO] - fit progress: (14, 0.1473388292312622, {'accuracy': 0.546}, 142.9049777850014)
[2025-02-10 15:42:39,575][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:39,611][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:39,611][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:39,612][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:42,281][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:42,282][flwr][INFO] - 
[2025-02-10 15:42:42,282][flwr][INFO] - [ROUND 15]
[2025-02-10 15:42:42,282][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:42,316][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:42,316][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:42,317][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:45,289][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:48,998][flwr][INFO] - fit progress: (15, 0.19445011422634126, {'accuracy': 0.5258}, 152.327746156996)
[2025-02-10 15:42:48,998][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:49,028][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:49,029][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:49,029][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:51,806][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:42:51,806][flwr][INFO] - 
[2025-02-10 15:42:51,806][flwr][INFO] - [ROUND 16]
[2025-02-10 15:42:51,806][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:51,834][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:51,834][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:51,835][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:42:55,016][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:42:58,673][flwr][INFO] - fit progress: (16, 0.2551432831764221, {'accuracy': 0.5039}, 162.00290514600056)
[2025-02-10 15:42:58,673][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:42:58,744][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:42:58,745][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:42:58,746][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:01,579][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:43:01,580][flwr][INFO] - 
[2025-02-10 15:43:01,580][flwr][INFO] - [ROUND 17]
[2025-02-10 15:43:01,580][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:01,651][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:01,651][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:01,652][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:04,189][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:43:08,088][flwr][INFO] - fit progress: (17, 0.3313620500564575, {'accuracy': 0.483}, 171.41736253099225)
[2025-02-10 15:43:08,088][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:08,166][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:08,166][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:08,167][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:11,498][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:43:11,498][flwr][INFO] - 
[2025-02-10 15:43:11,498][flwr][INFO] - [ROUND 18]
[2025-02-10 15:43:11,498][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:11,571][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:11,572][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:11,573][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:14,305][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:43:17,986][flwr][INFO] - fit progress: (18, 0.42735581455230714, {'accuracy': 0.4644}, 181.31618610399892)
[2025-02-10 15:43:17,987][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:18,090][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:18,090][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:18,091][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:20,393][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:43:20,393][flwr][INFO] - 
[2025-02-10 15:43:20,393][flwr][INFO] - [ROUND 19]
[2025-02-10 15:43:20,393][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:20,495][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:20,495][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:20,496][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:23,602][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:43:27,238][flwr][INFO] - fit progress: (19, 0.5442015301704407, {'accuracy': 0.4491}, 190.56760384399968)
[2025-02-10 15:43:27,238][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:27,308][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:27,308][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:27,309][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:30,647][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:43:30,648][flwr][INFO] - 
[2025-02-10 15:43:30,648][flwr][INFO] - [ROUND 20]
[2025-02-10 15:43:30,648][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:30,715][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:30,716][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:30,717][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:33,257][flwr][INFO] - aggregate_fit: received 9 results and 1 failures
[2025-02-10 15:43:36,900][flwr][INFO] - fit progress: (20, 0.6851759906768798, {'accuracy': 0.4353}, 200.22990544699132)
[2025-02-10 15:43:36,900][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:43:36,927][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:43:36,928][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:43:36,928][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 5bba9d149119783a47b7b118baa3bd17a5f82eb219210d30e17f0275) where the task (actor ID: 1961f80ffba141b5ebe8b56a01000000, name=ClientAppActor.__init__, pid=21185, memory used=0.58GB) was running was 7.24GB / 7.45GB (0.972684), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-551497f34b9684a0e18cd7232c98cd1275df769c0340834989271440*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
21178	0.58	ray::ClientAppActor.run
21181	0.58	ray::ClientAppActor.run
21185	0.58	ray::ClientAppActor.run
21184	0.58	ray::ClientAppActor.run
21182	0.57	ray::ClientAppActor.run
21180	0.57	ray::ClientAppActor.run
21179	0.56	ray::ClientAppActor.run
588	0.37	/home/theo_ubuntu/.vscode-server/bin/33fc5a94a3f99ebe7087e8fe79fbe1d37a251016/node --dns-result-orde...
21187	0.34	ray::ClientAppActor.run
21186	0.33	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:43:39,707][flwr][INFO] - aggregate_evaluate: received 9 results and 1 failures
[2025-02-10 15:43:39,708][flwr][INFO] - 
[2025-02-10 15:43:39,708][flwr][INFO] - [SUMMARY]
[2025-02-10 15:43:39,708][flwr][INFO] - Run finished 20 round(s) in 203.04s
[2025-02-10 15:43:39,715][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 1: 0.07639312589610064
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 2: 0.07574169635772704
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 3: 0.07428829979013513
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 4: 0.07098218655144727
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 5: 0.06488236851162381
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 6: 0.05681848459773594
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 7: 0.050189550386534804
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 8: 0.04849498338169522
[2025-02-10 15:43:39,715][flwr][INFO] - 		round 9: 0.05333660520889141
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 10: 0.06335508613674729
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 11: 0.07933088276121353
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 12: 0.10359510514471267
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 13: 0.13730448241587037
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 14: 0.18314404774595192
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 15: 0.24335865621213562
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 16: 0.319518796161369
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 17: 0.41426072738788744
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 18: 0.5310660600662231
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 19: 0.6745862007141112
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 20: 0.8450527676829586
[2025-02-10 15:43:39,716][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:43:39,716][flwr][INFO] - 		round 0: 0.0721090440750122
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 1: 0.07171215100288392
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 2: 0.07109681708812714
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 3: 0.06966230854988098
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 4: 0.06643684768676758
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 5: 0.06041395205259323
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 6: 0.05224622600078583
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 7: 0.045226541846990584
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 8: 0.042820585495233536
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 9: 0.04616017283797264
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 10: 0.053879778122901914
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 11: 0.06638471059203148
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 12: 0.08535963147878647
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 13: 0.11187102482318878
[2025-02-10 15:43:39,717][flwr][INFO] - 		round 14: 0.1473388292312622
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 15: 0.19445011422634126
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 16: 0.2551432831764221
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 17: 0.3313620500564575
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 18: 0.42735581455230714
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 19: 0.5442015301704407
[2025-02-10 15:43:39,718][flwr][INFO] - 		round 20: 0.6851759906768798
[2025-02-10 15:43:39,718][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:43:39,718][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867436),
[2025-02-10 15:43:39,718][flwr][INFO] - 	                      (2, 0.6813471387867436),
[2025-02-10 15:43:39,718][flwr][INFO] - 	                      (3, 0.6813471387867436),
[2025-02-10 15:43:39,718][flwr][INFO] - 	                      (4, 0.6813471387867436),
[2025-02-10 15:43:39,718][flwr][INFO] - 	                      (5, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (6, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (7, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (8, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (9, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (10, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (11, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (12, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (13, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (14, 0.6813471387867436),
[2025-02-10 15:43:39,719][flwr][INFO] - 	                      (15, 0.6813471387867436),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                      (16, 0.6813471387867436),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                      (17, 0.6813471387867436),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                      (18, 0.6813471387867436),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                      (19, 0.6813471387867436),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                      (20, 0.6813471387867436)],
[2025-02-10 15:43:39,720][flwr][INFO] - 	 'clients_losses': [(1, 2.298846384331032),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (2, 2.2843068370112665),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (3, 2.2572015515080204),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (4, 2.1959085323192453),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (5, 2.0680001815160116),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (6, 1.8638883219824898),
[2025-02-10 15:43:39,720][flwr][INFO] - 	                    (7, 1.6360098335478042),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (8, 1.4928166760338677),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (9, 1.5150759573336001),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (10, 1.7121265066994562),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (11, 2.064607732825809),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (12, 2.610430372202838),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (13, 3.408642778573213),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (14, 4.512299759299667),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (15, 5.968874157799615),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (16, 7.83190297903838),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (17, 10.167574045393202),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (18, 13.051781357659234),
[2025-02-10 15:43:39,721][flwr][INFO] - 	                    (19, 16.568037555835865),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                    (20, 20.77421349419488)]}
[2025-02-10 15:43:39,722][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:43:39,722][flwr][INFO] - 	{'clients_accuracies': [(1, 0.09351851851851851),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (2, 0.11944444444444444),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (3, 0.31574074074074077),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (4, 0.35925925925925933),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (5, 0.42500000000000004),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (6, 0.4851851851851852),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (7, 0.5212962962962964),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (8, 0.5444444444444444),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (9, 0.5611111111111111),
[2025-02-10 15:43:39,722][flwr][INFO] - 	                        (10, 0.5703703703703702),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (11, 0.5675925925925926),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (12, 0.549074074074074),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (13, 0.5324074074074074),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (14, 0.5111111111111111),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (15, 0.49074074074074076),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (16, 0.46388888888888885),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (17, 0.44351851851851853),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (18, 0.4268518518518518),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (19, 0.4101851851851851),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                        (20, 0.38888888888888884)],
[2025-02-10 15:43:39,723][flwr][INFO] - 	 'clients_losses': [(1, 0.07639312589610064),
[2025-02-10 15:43:39,723][flwr][INFO] - 	                    (2, 0.07574169635772704),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (3, 0.07428829979013513),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (4, 0.07098218655144727),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (5, 0.06488236851162381),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (6, 0.05681848459773594),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (7, 0.050189550386534804),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (8, 0.04849498338169522),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (9, 0.05333660520889141),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (10, 0.06335508613674729),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (11, 0.07933088276121353),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (12, 0.10359510514471267),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (13, 0.13730448241587037),
[2025-02-10 15:43:39,724][flwr][INFO] - 	                    (14, 0.18314404774595192),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (15, 0.24335865621213562),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (16, 0.319518796161369),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (17, 0.41426072738788744),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (18, 0.5310660600662231),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (19, 0.6745862007141112),
[2025-02-10 15:43:39,725][flwr][INFO] - 	                    (20, 0.8450527676829586)]}
[2025-02-10 15:43:39,725][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:43:39,725][flwr][INFO] - 	{'accuracy': [(0, 0.0788),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (1, 0.0981),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (2, 0.1262),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (3, 0.3167),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (4, 0.3638),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (5, 0.4264),
[2025-02-10 15:43:39,725][flwr][INFO] - 	              (6, 0.4974),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (7, 0.5314),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (8, 0.5517),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (9, 0.5643),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (10, 0.5777),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (11, 0.5813),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (12, 0.5774),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (13, 0.5651),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (14, 0.546),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (15, 0.5258),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (16, 0.5039),
[2025-02-10 15:43:39,726][flwr][INFO] - 	              (17, 0.483),
[2025-02-10 15:43:39,727][flwr][INFO] - 	              (18, 0.4644),
[2025-02-10 15:43:39,727][flwr][INFO] - 	              (19, 0.4491),
[2025-02-10 15:43:39,727][flwr][INFO] - 	              (20, 0.4353)]}
[2025-02-10 15:43:39,727][flwr][INFO] - 
