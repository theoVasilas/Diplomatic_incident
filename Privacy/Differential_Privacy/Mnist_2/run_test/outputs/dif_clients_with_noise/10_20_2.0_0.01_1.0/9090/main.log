[2025-02-10 15:59:14,434][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:59:14,435][flwr][INFO] - 
[2025-02-10 15:59:14,436][flwr][INFO] - [INIT]
[2025-02-10 15:59:14,436][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:59:14,436][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:59:20,547][flwr][INFO] - initial parameters (loss, other metrics): 0.07212486484050751, {'accuracy': 0.0958}
[2025-02-10 15:59:20,547][flwr][INFO] - 
[2025-02-10 15:59:20,547][flwr][INFO] - [ROUND 1]
[2025-02-10 15:59:20,547][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:25,506][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:25,506][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:25,519][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:25,520][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:25,580][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:25,581][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:30,906][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:59:35,247][flwr][INFO] - fit progress: (1, 0.07152827694416046, {'accuracy': 0.1565}, 14.699442602999625)
[2025-02-10 15:59:35,251][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:35,380][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:35,380][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:35,380][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:35,381][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:35,382][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:35,382][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:39,078][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:59:39,079][flwr][INFO] - 
[2025-02-10 15:59:39,079][flwr][INFO] - [ROUND 2]
[2025-02-10 15:59:39,080][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:39,158][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:39,159][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:39,160][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:39,161][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:39,161][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:39,162][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:41,787][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:59:46,142][flwr][INFO] - fit progress: (2, 0.07059135971069336, {'accuracy': 0.2239}, 25.594897234987002)
[2025-02-10 15:59:46,142][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:46,172][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:46,172][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:46,172][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:46,172][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:46,174][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:46,175][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:49,149][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:59:49,149][flwr][INFO] - 
[2025-02-10 15:59:49,149][flwr][INFO] - [ROUND 3]
[2025-02-10 15:59:49,149][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:49,175][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:49,177][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:49,177][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:49,179][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:49,180][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:49,180][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:51,756][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:59:55,849][flwr][INFO] - fit progress: (3, 0.0682367192029953, {'accuracy': 0.2272}, 35.30198679899331)
[2025-02-10 15:59:55,849][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:55,891][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:55,892][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:55,893][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:55,892][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:55,893][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:55,894][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:59:58,957][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:59:58,958][flwr][INFO] - 
[2025-02-10 15:59:58,958][flwr][INFO] - [ROUND 4]
[2025-02-10 15:59:58,958][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:59:58,999][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:59,000][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 15:59:59,000][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:59:59,001][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:59:59,002][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 15:59:59,003][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:01,665][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:05,793][flwr][INFO] - fit progress: (4, 0.0636075337767601, {'accuracy': 0.3421}, 45.245621125999605)
[2025-02-10 16:00:05,793][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:05,904][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:05,905][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:05,905][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:05,906][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:05,906][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:05,907][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:08,906][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:08,906][flwr][INFO] - 
[2025-02-10 16:00:08,906][flwr][INFO] - [ROUND 5]
[2025-02-10 16:00:08,906][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:09,015][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:09,015][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:09,016][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:09,015][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:09,017][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:09,017][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:11,815][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:15,914][flwr][INFO] - fit progress: (5, 0.05630333704948425, {'accuracy': 0.4898}, 55.3671533799934)
[2025-02-10 16:00:15,914][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:16,026][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:16,028][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:16,028][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:16,029][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:16,028][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:16,030][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:18,923][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:18,924][flwr][INFO] - 
[2025-02-10 16:00:18,924][flwr][INFO] - [ROUND 6]
[2025-02-10 16:00:18,924][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:19,032][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:19,033][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:19,034][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:19,034][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:19,035][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:19,035][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:22,034][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:25,809][flwr][INFO] - fit progress: (6, 0.048256796073913574, {'accuracy': 0.5198}, 65.26234257398755)
[2025-02-10 16:00:25,810][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:25,845][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:25,845][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:25,846][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:25,845][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:25,846][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:25,847][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:29,017][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:29,017][flwr][INFO] - 
[2025-02-10 16:00:29,017][flwr][INFO] - [ROUND 7]
[2025-02-10 16:00:29,017][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:29,049][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:29,050][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:29,050][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:29,051][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:29,051][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:29,052][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:31,725][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:35,830][flwr][INFO] - fit progress: (7, 0.04311481632590294, {'accuracy': 0.5306}, 75.2831563159998)
[2025-02-10 16:00:35,830][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:35,861][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:35,862][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:35,863][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:35,863][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:35,865][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:35,864][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:38,539][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:38,539][flwr][INFO] - 
[2025-02-10 16:00:38,539][flwr][INFO] - [ROUND 8]
[2025-02-10 16:00:38,539][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:38,567][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:38,567][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:38,568][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:38,569][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:38,569][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:38,570][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:41,246][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:45,113][flwr][INFO] - fit progress: (8, 0.041730669474601745, {'accuracy': 0.547}, 84.56634524298715)
[2025-02-10 16:00:45,114][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:45,179][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:45,179][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:45,180][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:45,181][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:45,181][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:45,181][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:48,122][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:48,122][flwr][INFO] - 
[2025-02-10 16:00:48,122][flwr][INFO] - [ROUND 9]
[2025-02-10 16:00:48,123][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:48,184][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:48,184][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:48,185][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:48,186][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:48,187][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:48,187][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:51,031][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:00:54,693][flwr][INFO] - fit progress: (9, 0.044678336077928545, {'accuracy': 0.5616}, 94.14629985998909)
[2025-02-10 16:00:54,694][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:54,796][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:54,797][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:54,798][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:54,798][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:54,799][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:54,800][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:00:57,701][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:00:57,702][flwr][INFO] - 
[2025-02-10 16:00:57,702][flwr][INFO] - [ROUND 10]
[2025-02-10 16:00:57,702][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:00:57,804][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:57,804][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:00:57,804][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:00:57,805][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:00:57,806][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:00:57,806][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:00,610][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:04,302][flwr][INFO] - fit progress: (10, 0.05297179748415947, {'accuracy': 0.5582}, 103.75518207999994)
[2025-02-10 16:01:04,302][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:04,417][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:04,418][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:04,419][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:04,419][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:04,420][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:04,420][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:08,014][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:08,015][flwr][INFO] - 
[2025-02-10 16:01:08,015][flwr][INFO] - [ROUND 11]
[2025-02-10 16:01:08,015][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:08,124][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:08,124][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:08,125][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:08,125][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:08,126][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:08,126][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:10,821][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:14,760][flwr][INFO] - fit progress: (11, 0.06620713148117065, {'accuracy': 0.5499}, 114.21324159098731)
[2025-02-10 16:01:14,761][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:14,836][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:14,836][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:14,836][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:14,838][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:14,836][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:14,839][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:18,170][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:18,171][flwr][INFO] - 
[2025-02-10 16:01:18,171][flwr][INFO] - [ROUND 12]
[2025-02-10 16:01:18,171][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:18,242][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:18,243][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:18,244][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:18,245][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:18,245][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:18,245][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:20,878][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:24,596][flwr][INFO] - fit progress: (12, 0.08528361546993256, {'accuracy': 0.5387}, 124.0492539859988)
[2025-02-10 16:01:24,597][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:24,653][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:24,653][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:24,654][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:24,654][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:24,654][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:24,656][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:27,405][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:27,405][flwr][INFO] - 
[2025-02-10 16:01:27,406][flwr][INFO] - [ROUND 13]
[2025-02-10 16:01:27,406][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:27,460][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:27,460][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:27,461][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:27,462][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:27,462][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:27,463][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:30,013][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:34,142][flwr][INFO] - fit progress: (13, 0.11061160312891007, {'accuracy': 0.5261}, 133.59463488298934)
[2025-02-10 16:01:34,142][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:34,170][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:34,170][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:34,172][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:34,171][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:34,173][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:34,173][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:37,950][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:37,950][flwr][INFO] - 
[2025-02-10 16:01:37,950][flwr][INFO] - [ROUND 14]
[2025-02-10 16:01:37,950][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:37,979][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:37,979][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:37,980][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:37,981][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:37,981][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:37,981][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:40,659][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:44,339][flwr][INFO] - fit progress: (14, 0.14342229261398315, {'accuracy': 0.514}, 143.7920150049904)
[2025-02-10 16:01:44,339][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:44,390][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:44,390][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:44,390][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:44,391][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:44,392][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:44,392][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:47,346][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:47,346][flwr][INFO] - 
[2025-02-10 16:01:47,346][flwr][INFO] - [ROUND 15]
[2025-02-10 16:01:47,347][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:47,394][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:47,395][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:47,395][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:47,397][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:47,398][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:47,399][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:50,058][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:01:54,130][flwr][INFO] - fit progress: (15, 0.18539170060157775, {'accuracy': 0.5018}, 153.58309314699727)
[2025-02-10 16:01:54,130][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:54,207][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:54,208][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:54,209][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:54,209][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:54,210][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:54,211][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:57,137][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:01:57,138][flwr][INFO] - 
[2025-02-10 16:01:57,138][flwr][INFO] - [ROUND 16]
[2025-02-10 16:01:57,138][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:01:57,214][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:57,215][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:01:57,215][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:01:57,216][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:01:57,216][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:01:57,218][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:01:59,745][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:02:03,542][flwr][INFO] - fit progress: (16, 0.23810333919525145, {'accuracy': 0.4891}, 162.99497053299274)
[2025-02-10 16:02:03,542][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:03,627][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:03,628][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:03,629][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:03,629][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:03,629][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:03,630][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:06,550][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:02:06,551][flwr][INFO] - 
[2025-02-10 16:02:06,551][flwr][INFO] - [ROUND 17]
[2025-02-10 16:02:06,551][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:06,633][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:06,634][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:06,635][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:06,636][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:06,636][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:06,637][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:09,258][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:02:12,988][flwr][INFO] - fit progress: (17, 0.30444131443500516, {'accuracy': 0.4784}, 172.44055422199017)
[2025-02-10 16:02:12,988][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:13,046][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:13,046][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:13,047][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:13,047][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:13,048][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:13,048][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:15,794][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:02:15,794][flwr][INFO] - 
[2025-02-10 16:02:15,794][flwr][INFO] - [ROUND 18]
[2025-02-10 16:02:15,795][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:15,850][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:15,851][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:15,852][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:15,852][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:15,852][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:15,854][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:18,602][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:02:22,373][flwr][INFO] - fit progress: (18, 0.38618860430717467, {'accuracy': 0.4653}, 181.82545792999736)
[2025-02-10 16:02:22,373][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:22,462][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:22,464][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:22,464][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:22,464][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:22,465][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:22,466][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:25,579][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:02:25,579][flwr][INFO] - 
[2025-02-10 16:02:25,579][flwr][INFO] - [ROUND 19]
[2025-02-10 16:02:25,579][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:25,669][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:25,669][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:25,670][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:25,670][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:25,671][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:25,672][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:28,386][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:02:32,297][flwr][INFO] - fit progress: (19, 0.48730415320396425, {'accuracy': 0.4515}, 191.7499189079972)
[2025-02-10 16:02:32,297][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:32,383][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:32,385][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:32,385][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:32,386][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:32,386][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:32,387][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:35,305][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:02:35,305][flwr][INFO] - 
[2025-02-10 16:02:35,305][flwr][INFO] - [ROUND 20]
[2025-02-10 16:02:35,305][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:35,388][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:35,388][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:35,388][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:35,388][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:35,389][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:35,390][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:38,113][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 16:02:41,899][flwr][INFO] - fit progress: (20, 0.6089291283607483, {'accuracy': 0.4445}, 201.35190049199446)
[2025-02-10 16:02:41,899][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 16:02:42,000][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:42,001][flwr][ERROR] - The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.
[2025-02-10 16:02:42,001][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 16:02:42,002][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 16:02:42,002][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.
	class_name: ClientAppActor
	actor_id: 20a02588df6778b0d00a8f7c01000000
	pid: 19196
	namespace: 09ed26bd-1a30-4f5e-9f3a-8ba346bf92ab
	ip: 172.27.85.201
The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.

[2025-02-10 16:02:42,003][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: f3031247c6bd18361ff10df2a9b6092ae686365fa763ac801e9f4c3f) where the task (actor ID: 5323b689b2fa9ea432f8412501000000, name=ClientAppActor.__init__, pid=19197, memory used=0.50GB) was running was 7.12GB / 7.45GB (0.955666), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-d942e7c8c829ed394ee47f465113c93ef8c820385b5da28fbf27846b*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
19196	0.56	ray::ClientAppActor.run
19198	0.53	ray::ClientAppActor.run
19197	0.50	ray::ClientAppActor.run
19193	0.49	ray::ClientAppActor.run
19191	0.49	ray::ClientAppActor.run
19195	0.49	ray::ClientAppActor.run
19199	0.48	ray::ClientAppActor.run
19200	0.48	ray::ClientAppActor.run
19194	0.46	ray::ClientAppActor.run
19192	0.44	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 16:02:45,005][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 16:02:45,008][flwr][INFO] - 
[2025-02-10 16:02:45,008][flwr][INFO] - [SUMMARY]
[2025-02-10 16:02:45,008][flwr][INFO] - Run finished 20 round(s) in 204.46s
[2025-02-10 16:02:45,015][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 16:02:45,015][flwr][INFO] - 		round 1: 0.07621687526504198
[2025-02-10 16:02:45,015][flwr][INFO] - 		round 2: 0.07517803733547529
[2025-02-10 16:02:45,015][flwr][INFO] - 		round 3: 0.07260743528604507
[2025-02-10 16:02:45,015][flwr][INFO] - 		round 4: 0.06768491591016453
[2025-02-10 16:02:45,015][flwr][INFO] - 		round 5: 0.06026424666245778
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 6: 0.05231299760440984
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 7: 0.047650379066665964
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 8: 0.04704530512293179
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 9: 0.05129005443304777
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 10: 0.06151277348399162
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 11: 0.07728026236097016
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 12: 0.09962737349172433
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 13: 0.1286713510751724
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 14: 0.16609577909111978
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 15: 0.21277706747253736
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 16: 0.2713864969710509
[2025-02-10 16:02:45,016][flwr][INFO] - 		round 17: 0.34482923348744715
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 18: 0.4346826319893201
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 19: 0.5446173886458079
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 20: 0.6757614751656851
[2025-02-10 16:02:45,017][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 0: 0.07212486484050751
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 1: 0.07152827694416046
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 2: 0.07059135971069336
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 3: 0.0682367192029953
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 4: 0.0636075337767601
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 5: 0.05630333704948425
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 6: 0.048256796073913574
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 7: 0.04311481632590294
[2025-02-10 16:02:45,017][flwr][INFO] - 		round 8: 0.041730669474601745
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 9: 0.044678336077928545
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 10: 0.05297179748415947
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 11: 0.06620713148117065
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 12: 0.08528361546993256
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 13: 0.11061160312891007
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 14: 0.14342229261398315
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 15: 0.18539170060157775
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 16: 0.23810333919525145
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 17: 0.30444131443500516
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 18: 0.38618860430717467
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 19: 0.48730415320396425
[2025-02-10 16:02:45,018][flwr][INFO] - 		round 20: 0.6089291283607483
[2025-02-10 16:02:45,018][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 16:02:45,018][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (2, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (3, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (4, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (5, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (6, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (7, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (8, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (9, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (10, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (11, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (12, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (13, 0.6813471387867437),
[2025-02-10 16:02:45,019][flwr][INFO] - 	                      (14, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (15, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (16, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (17, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (18, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (19, 0.6813471387867437),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                      (20, 0.6813471387867437)],
[2025-02-10 16:02:45,020][flwr][INFO] - 	 'clients_losses': [(1, 2.2987713237603504),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                    (2, 2.276601429780324),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                    (3, 2.2334113121032715),
[2025-02-10 16:02:45,020][flwr][INFO] - 	                    (4, 2.1380910257498424),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (5, 1.972263311346372),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (6, 1.7492234577735266),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (7, 1.5586808264255523),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (8, 1.4788028955459596),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (9, 1.5135528107484182),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (10, 1.7015461554129918),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (11, 2.059271663427353),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (12, 2.598357450962067),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (13, 3.3233401507139204),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (14, 4.2758559366067255),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (15, 5.490343701839447),
[2025-02-10 16:02:45,021][flwr][INFO] - 	                    (16, 7.027851808071136),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                    (17, 8.980705118179321),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                    (18, 11.421407810846965),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                    (19, 14.430687165260315),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                    (20, 18.119866959253947)]}
[2025-02-10 16:02:45,022][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 16:02:45,022][flwr][INFO] - 	{'clients_accuracies': [(1, 0.14583333333333331),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (2, 0.22291666666666665),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (3, 0.24375),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (4, 0.37395833333333334),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (5, 0.4822916666666667),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (6, 0.5104166666666666),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (7, 0.5166666666666666),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (8, 0.5177083333333332),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (9, 0.528125),
[2025-02-10 16:02:45,022][flwr][INFO] - 	                        (10, 0.5291666666666666),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (11, 0.5260416666666666),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (12, 0.503125),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (13, 0.4927083333333334),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (14, 0.4822916666666667),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (15, 0.4572916666666666),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (16, 0.44791666666666663),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (17, 0.44375),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (18, 0.44270833333333326),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (19, 0.4239583333333333),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                        (20, 0.421875)],
[2025-02-10 16:02:45,023][flwr][INFO] - 	 'clients_losses': [(1, 0.07621687526504198),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                    (2, 0.07517803733547529),
[2025-02-10 16:02:45,023][flwr][INFO] - 	                    (3, 0.07260743528604507),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (4, 0.06768491591016453),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (5, 0.06026424666245778),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (6, 0.05231299760440984),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (7, 0.047650379066665964),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (8, 0.04704530512293179),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (9, 0.05129005443304777),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (10, 0.06151277348399162),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (11, 0.07728026236097016),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (12, 0.09962737349172433),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (13, 0.1286713510751724),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (14, 0.16609577909111978),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (15, 0.21277706747253736),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (16, 0.2713864969710509),
[2025-02-10 16:02:45,024][flwr][INFO] - 	                    (17, 0.34482923348744715),
[2025-02-10 16:02:45,025][flwr][INFO] - 	                    (18, 0.4346826319893201),
[2025-02-10 16:02:45,025][flwr][INFO] - 	                    (19, 0.5446173886458079),
[2025-02-10 16:02:45,025][flwr][INFO] - 	                    (20, 0.6757614751656851)]}
[2025-02-10 16:02:45,025][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 16:02:45,025][flwr][INFO] - 	{'accuracy': [(0, 0.0958),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (1, 0.1565),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (2, 0.2239),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (3, 0.2272),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (4, 0.3421),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (5, 0.4898),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (6, 0.5198),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (7, 0.5306),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (8, 0.547),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (9, 0.5616),
[2025-02-10 16:02:45,025][flwr][INFO] - 	              (10, 0.5582),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (11, 0.5499),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (12, 0.5387),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (13, 0.5261),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (14, 0.514),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (15, 0.5018),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (16, 0.4891),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (17, 0.4784),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (18, 0.4653),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (19, 0.4515),
[2025-02-10 16:02:45,026][flwr][INFO] - 	              (20, 0.4445)]}
[2025-02-10 16:02:45,026][flwr][INFO] - 
