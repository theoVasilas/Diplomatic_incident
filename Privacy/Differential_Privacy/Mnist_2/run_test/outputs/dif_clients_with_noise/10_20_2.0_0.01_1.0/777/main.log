[2025-02-10 15:51:36,745][flwr][INFO] - Starting Flower ServerApp, config: num_rounds=20, no round_timeout
[2025-02-10 15:51:36,745][flwr][INFO] - 
[2025-02-10 15:51:36,746][flwr][INFO] - [INIT]
[2025-02-10 15:51:36,747][flwr][INFO] - Using initial global parameters provided by strategy
[2025-02-10 15:51:36,747][flwr][INFO] - Starting evaluation of initial global parameters
[2025-02-10 15:51:42,848][flwr][INFO] - initial parameters (loss, other metrics): 0.07224980416297913, {'accuracy': 0.0822}
[2025-02-10 15:51:42,848][flwr][INFO] - 
[2025-02-10 15:51:42,848][flwr][INFO] - [ROUND 1]
[2025-02-10 15:51:42,849][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:51:47,838][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:51:47,865][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:51:47,965][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:51:48,002][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:51:48,002][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:51:48,004][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:51:53,227][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:51:57,734][flwr][INFO] - fit progress: (1, 0.07148284475803375, {'accuracy': 0.1349}, 14.88588295999216)
[2025-02-10 15:51:57,738][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:51:57,814][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:51:57,815][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:51:57,815][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:51:57,816][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:51:57,816][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:51:57,818][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:00,947][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:00,949][flwr][INFO] - 
[2025-02-10 15:52:00,949][flwr][INFO] - [ROUND 2]
[2025-02-10 15:52:00,949][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:00,991][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:00,991][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:00,992][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:00,991][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:00,993][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:00,993][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:03,468][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:07,497][flwr][INFO] - fit progress: (2, 0.07020996594429016, {'accuracy': 0.1302}, 24.64835296999081)
[2025-02-10 15:52:07,497][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:07,602][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:07,604][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:07,604][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:07,604][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:07,605][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:07,605][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:10,204][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:10,204][flwr][INFO] - 
[2025-02-10 15:52:10,205][flwr][INFO] - [ROUND 3]
[2025-02-10 15:52:10,205][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:10,306][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:10,308][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:10,308][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:10,309][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:10,309][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:10,311][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:12,913][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:16,755][flwr][INFO] - fit progress: (3, 0.06766377944946289, {'accuracy': 0.2749}, 33.90641779999714)
[2025-02-10 15:52:16,755][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:16,820][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:16,820][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:16,821][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:16,821][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:16,821][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:16,822][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:19,662][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:19,663][flwr][INFO] - 
[2025-02-10 15:52:19,663][flwr][INFO] - [ROUND 4]
[2025-02-10 15:52:19,663][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:19,724][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:19,727][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:19,728][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:19,728][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:19,731][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:19,731][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:22,373][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:26,525][flwr][INFO] - fit progress: (4, 0.0626206129193306, {'accuracy': 0.4119}, 43.676714973989874)
[2025-02-10 15:52:26,525][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:26,636][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:26,637][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:26,637][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:26,637][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:26,638][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:26,638][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:28,837][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:28,837][flwr][INFO] - 
[2025-02-10 15:52:28,838][flwr][INFO] - [ROUND 5]
[2025-02-10 15:52:28,838][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:28,945][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:28,945][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:28,946][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:28,948][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:28,947][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:28,949][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:31,445][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:35,393][flwr][INFO] - fit progress: (5, 0.054857202887535095, {'accuracy': 0.4769}, 52.54471254500095)
[2025-02-10 15:52:35,393][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:35,458][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:35,460][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:35,461][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:35,461][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:35,462][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:35,462][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:37,707][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:37,707][flwr][INFO] - 
[2025-02-10 15:52:37,707][flwr][INFO] - [ROUND 6]
[2025-02-10 15:52:37,707][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:37,763][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:37,765][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:37,766][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:37,766][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:37,767][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:37,768][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:40,315][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:44,254][flwr][INFO] - fit progress: (6, 0.04622156492471695, {'accuracy': 0.5355}, 61.405714822991285)
[2025-02-10 15:52:44,254][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:44,278][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:44,279][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:44,280][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:44,280][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:44,281][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:44,281][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:47,361][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:47,361][flwr][INFO] - 
[2025-02-10 15:52:47,361][flwr][INFO] - [ROUND 7]
[2025-02-10 15:52:47,361][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:47,382][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:47,386][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:47,388][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:47,388][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:47,389][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:47,390][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:50,870][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:52:54,597][flwr][INFO] - fit progress: (7, 0.04078794264793396, {'accuracy': 0.5647}, 71.74916671299434)
[2025-02-10 15:52:54,598][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:54,692][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:54,695][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:54,696][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:54,696][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:54,696][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:54,698][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:57,305][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:52:57,305][flwr][INFO] - 
[2025-02-10 15:52:57,306][flwr][INFO] - [ROUND 8]
[2025-02-10 15:52:57,306][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:52:57,400][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:57,402][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:52:57,402][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:57,402][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:52:57,403][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:57,404][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:52:59,913][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:03,604][flwr][INFO] - fit progress: (8, 0.041281289744377135, {'accuracy': 0.5795}, 80.75595284299925)
[2025-02-10 15:53:03,605][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:03,714][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:03,716][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:03,715][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:03,716][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:03,717][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:03,718][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:06,512][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:06,512][flwr][INFO] - 
[2025-02-10 15:53:06,512][flwr][INFO] - [ROUND 9]
[2025-02-10 15:53:06,513][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:06,620][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:06,621][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:06,622][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:06,621][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:06,622][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:06,623][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:09,122][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:13,120][flwr][INFO] - fit progress: (9, 0.047849865555763246, {'accuracy': 0.5877}, 90.27216041499923)
[2025-02-10 15:53:13,121][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:13,232][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:13,232][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:13,232][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:13,233][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:13,233][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:13,234][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:15,629][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:15,629][flwr][INFO] - 
[2025-02-10 15:53:15,629][flwr][INFO] - [ROUND 10]
[2025-02-10 15:53:15,629][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:15,738][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:15,739][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:15,739][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:15,738][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:15,740][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:15,741][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:18,337][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:22,020][flwr][INFO] - fit progress: (10, 0.05972652862071991, {'accuracy': 0.5879}, 99.17231299399282)
[2025-02-10 15:53:22,021][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:22,050][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:22,050][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:22,051][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:22,051][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:22,051][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:22,052][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:24,827][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:24,827][flwr][INFO] - 
[2025-02-10 15:53:24,827][flwr][INFO] - [ROUND 11]
[2025-02-10 15:53:24,828][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:24,855][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:24,856][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:24,857][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:24,858][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:24,858][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:24,859][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:27,634][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:31,345][flwr][INFO] - fit progress: (11, 0.07793749681711197, {'accuracy': 0.5844}, 108.49720929299656)
[2025-02-10 15:53:31,346][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:31,367][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:31,367][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:31,368][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:31,368][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:31,367][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:31,369][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:33,651][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:33,651][flwr][INFO] - 
[2025-02-10 15:53:33,651][flwr][INFO] - [ROUND 12]
[2025-02-10 15:53:33,652][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:33,670][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:33,671][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:33,673][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:33,673][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:33,674][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:33,674][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:36,358][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:40,164][flwr][INFO] - fit progress: (12, 0.10290517466068268, {'accuracy': 0.5753}, 117.31604521498957)
[2025-02-10 15:53:40,164][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:40,274][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:40,274][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:40,274][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:40,275][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:40,275][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:40,276][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:42,577][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:42,577][flwr][INFO] - 
[2025-02-10 15:53:42,577][flwr][INFO] - [ROUND 13]
[2025-02-10 15:53:42,578][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:42,685][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:42,685][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:42,687][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:42,688][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:42,688][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:42,689][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:46,386][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:53:50,876][flwr][INFO] - fit progress: (13, 0.13689554110765456, {'accuracy': 0.5587}, 128.02777706299094)
[2025-02-10 15:53:50,876][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:50,900][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:50,900][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:50,901][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:50,901][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:50,902][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:50,903][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:53,583][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:53:53,583][flwr][INFO] - 
[2025-02-10 15:53:53,583][flwr][INFO] - [ROUND 14]
[2025-02-10 15:53:53,583][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:53:53,605][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:53,606][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:53:53,607][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:53,607][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:53:53,608][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:53,608][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:53:56,092][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:00,116][flwr][INFO] - fit progress: (14, 0.18238952922821045, {'accuracy': 0.5421}, 137.26788295799633)
[2025-02-10 15:54:00,116][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:00,215][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:00,216][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:00,217][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:00,218][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:00,219][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:00,220][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:03,226][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:03,226][flwr][INFO] - 
[2025-02-10 15:54:03,227][flwr][INFO] - [ROUND 15]
[2025-02-10 15:54:03,227][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:03,323][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:03,324][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:03,325][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:03,324][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:03,326][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:03,326][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:05,938][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:10,014][flwr][INFO] - fit progress: (15, 0.2411556024312973, {'accuracy': 0.5254}, 147.16593192100117)
[2025-02-10 15:54:10,014][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:10,036][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:10,036][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:10,037][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:10,038][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:10,038][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:10,039][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:13,022][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:13,023][flwr][INFO] - 
[2025-02-10 15:54:13,023][flwr][INFO] - [ROUND 16]
[2025-02-10 15:54:13,023][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:13,130][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:13,131][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:13,131][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:13,131][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:13,132][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:13,132][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:15,631][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:19,584][flwr][INFO] - fit progress: (16, 0.31452212743759156, {'accuracy': 0.5135}, 156.73631120599748)
[2025-02-10 15:54:19,585][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:19,649][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:19,651][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:19,653][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:19,652][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:19,654][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:19,654][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:22,592][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:22,592][flwr][INFO] - 
[2025-02-10 15:54:22,592][flwr][INFO] - [ROUND 17]
[2025-02-10 15:54:22,592][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:22,658][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:22,658][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:22,659][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:22,659][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:22,660][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:22,661][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:25,401][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:29,423][flwr][INFO] - fit progress: (17, 0.4075912612915039, {'accuracy': 0.4942}, 166.57508290398982)
[2025-02-10 15:54:29,423][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:29,470][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:29,471][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:29,472][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:29,472][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:29,473][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:29,474][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:32,230][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:32,230][flwr][INFO] - 
[2025-02-10 15:54:32,230][flwr][INFO] - [ROUND 18]
[2025-02-10 15:54:32,231][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:32,277][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:32,277][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:32,278][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:32,277][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:32,279][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:32,279][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:34,836][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:38,575][flwr][INFO] - fit progress: (18, 0.5202660118579865, {'accuracy': 0.4782}, 175.72636769499513)
[2025-02-10 15:54:38,575][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:38,688][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:38,689][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:38,689][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:38,690][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:38,689][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:38,691][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:41,485][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:41,486][flwr][INFO] - 
[2025-02-10 15:54:41,486][flwr][INFO] - [ROUND 19]
[2025-02-10 15:54:41,486][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:41,595][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:41,596][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:41,597][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:41,596][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:41,597][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:41,598][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:44,593][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:48,352][flwr][INFO] - fit progress: (19, 0.6588899305343628, {'accuracy': 0.4627}, 185.50421789099346)
[2025-02-10 15:54:48,353][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:48,413][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:48,415][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:48,415][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:48,415][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:48,416][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:48,417][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:51,365][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:54:51,366][flwr][INFO] - 
[2025-02-10 15:54:51,366][flwr][INFO] - [ROUND 20]
[2025-02-10 15:54:51,366][flwr][INFO] - configure_fit: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:51,418][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:51,419][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:51,421][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:51,421][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:51,422][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:51,424][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:54,573][flwr][INFO] - aggregate_fit: received 8 results and 2 failures
[2025-02-10 15:54:58,537][flwr][INFO] - fit progress: (20, 0.82352120885849, {'accuracy': 0.4491}, 195.6883830989973)
[2025-02-10 15:54:58,537][flwr][INFO] - configure_evaluate: strategy sampled 10 clients (out of 10)
[2025-02-10 15:54:58,632][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:58,632][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:58,633][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: bd1a76937f5b54d96e27791c01000000, name=ClientAppActor.__init__, pid=7281, memory used=0.58GB) was running was 7.38GB / 7.45GB (0.990565), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-6873c824c715f2f8810a34d241a24f15fd7e14860b536f6b0a80ad30*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7275	0.58	ray::ClientAppActor.run
7281	0.58	ray::ClientAppActor.run
7277	0.58	ray::ClientAppActor.run
7274	0.58	ray::ClientAppActor.run
7279	0.58	ray::ClientAppActor.run
7280	0.57	ray::ClientAppActor.run
7282	0.57	ray::ClientAppActor.run
7276	0.57	ray::ClientAppActor.run
7278	0.57	ray::ClientAppActor.run
6667	0.22	/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/core/src/ray/raylet/ra...
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:54:58,633][flwr][ERROR] - An exception was raised when processing a message by RayBackend
[2025-02-10 15:54:58,633][flwr][ERROR] - Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
[2025-02-10 15:54:58,634][flwr][ERROR] - Traceback (most recent call last):
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/vce_api.py", line 112, in worker
    out_mssg, updated_context = backend.process_message(message, context)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 187, in process_message
    raise ex
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/server/superlink/fleet/vce/backend/raybackend.py", line 175, in process_message
    ) = self.pool.fetch_result_and_return_actor_to_pool(future)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/flwr/simulation/ray_transport/ray_actor.py", line 478, in fetch_result_and_return_actor_to_pool
    _, out_mssg, updated_context = ray.get(future)
                                   ^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 2667, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/theo_ubuntu/miniconda3/envs/flwr_25_DP/lib/python3.11/site-packages/ray/_private/worker.py", line 866, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.
Memory on the node (IP: 172.27.85.201, ID: 7881f8221acb92dd700f55f57bcdc9ec3c2bec3f59b6f0d4a5661397) where the task (actor ID: 50d2533b7de05bec6bad029001000000, name=ClientAppActor.__init__, pid=7283, memory used=0.48GB) was running was 7.16GB / 7.45GB (0.961489), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.27.85.201`. To see the logs of the worker, use `ray logs worker-5e1ab264c3c47d4f444f23e4895016b60fc7e787c6548cecfd5e6932*out -ip 172.27.85.201. Top 10 memory users:
PID	MEM(GB)	COMMAND
7281	0.51	ray::ClientAppActor.run
7277	0.50	ray::ClientAppActor.run
7282	0.50	ray::ClientAppActor.run
7278	0.50	ray::ClientAppActor.run
7280	0.50	ray::ClientAppActor.run
7275	0.50	ray::ClientAppActor.run
7283	0.48	ray::ClientAppActor.run
7279	0.47	ray::ClientAppActor.run
7274	0.46	ray::ClientAppActor.run
7276	0.40	ray::ClientAppActor.run
Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

[2025-02-10 15:55:01,447][flwr][INFO] - aggregate_evaluate: received 8 results and 2 failures
[2025-02-10 15:55:01,447][flwr][INFO] - 
[2025-02-10 15:55:01,447][flwr][INFO] - [SUMMARY]
[2025-02-10 15:55:01,447][flwr][INFO] - Run finished 20 round(s) in 198.60s
[2025-02-10 15:55:01,453][flwr][INFO] - 	History (loss, distributed):
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 1: 0.07601131101449332
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 2: 0.07467826555172602
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 3: 0.07195077960689863
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 4: 0.06655760382612545
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 5: 0.058172297105193144
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 6: 0.048887712632616355
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 7: 0.04282930462310712
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 8: 0.042815215326845645
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 9: 0.048677815869450566
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 10: 0.060247592255473145
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 11: 0.07868031815936169
[2025-02-10 15:55:01,454][flwr][INFO] - 		round 12: 0.10755089869101842
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 13: 0.14107166578372318
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 14: 0.18924244319399197
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 15: 0.25119172806541123
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 16: 0.3287573779622714
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 17: 0.4254591777920722
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 18: 0.5449563483397165
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 19: 0.6896040717760721
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 20: 0.8627415070931117
[2025-02-10 15:55:01,455][flwr][INFO] - 	History (loss, centralized):
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 0: 0.07224980416297913
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 1: 0.07148284475803375
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 2: 0.07020996594429016
[2025-02-10 15:55:01,455][flwr][INFO] - 		round 3: 0.06766377944946289
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 4: 0.0626206129193306
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 5: 0.054857202887535095
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 6: 0.04622156492471695
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 7: 0.04078794264793396
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 8: 0.041281289744377135
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 9: 0.047849865555763246
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 10: 0.05972652862071991
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 11: 0.07793749681711197
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 12: 0.10290517466068268
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 13: 0.13689554110765456
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 14: 0.18238952922821045
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 15: 0.2411556024312973
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 16: 0.31452212743759156
[2025-02-10 15:55:01,456][flwr][INFO] - 		round 17: 0.4075912612915039
[2025-02-10 15:55:01,457][flwr][INFO] - 		round 18: 0.5202660118579865
[2025-02-10 15:55:01,457][flwr][INFO] - 		round 19: 0.6588899305343628
[2025-02-10 15:55:01,457][flwr][INFO] - 		round 20: 0.82352120885849
[2025-02-10 15:55:01,457][flwr][INFO] - 	History (metrics, distributed, fit):
[2025-02-10 15:55:01,457][flwr][INFO] - 	{'clients_epsilons': [(1, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (2, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (3, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (4, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (5, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (6, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (7, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (8, 0.6813471387867437),
[2025-02-10 15:55:01,457][flwr][INFO] - 	                      (9, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (10, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (11, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (12, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (13, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (14, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (15, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (16, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (17, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (18, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (19, 0.6813471387867437),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                      (20, 0.6813471387867437)],
[2025-02-10 15:55:01,458][flwr][INFO] - 	 'clients_losses': [(1, 2.2959766328334807),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                    (2, 2.2693272213141125),
[2025-02-10 15:55:01,458][flwr][INFO] - 	                    (3, 2.216031297047933),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (4, 2.11336623330911),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (5, 1.931586257616679),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (6, 1.6900987118482589),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (7, 1.4748048494259516),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (8, 1.3942218899726868),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (9, 1.4871364027261733),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (10, 1.7655976295471192),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (11, 2.2334003125627837),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (12, 2.9226530849933625),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (13, 3.8820086856683096),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (14, 5.168106956283252),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (15, 6.849157633384069),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (16, 8.94524656534195),
[2025-02-10 15:55:01,459][flwr][INFO] - 	                    (17, 11.694477951526641),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                    (18, 14.986488246917725),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                    (19, 19.037018934885662),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                    (20, 23.900796659787495)]}
[2025-02-10 15:55:01,460][flwr][INFO] - 	History (metrics, distributed, evaluate):
[2025-02-10 15:55:01,460][flwr][INFO] - 	{'clients_accuracies': [(1, 0.14375),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (2, 0.13124999999999998),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (3, 0.2875),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (4, 0.42916666666666664),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (5, 0.47291666666666676),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (6, 0.5447916666666667),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (7, 0.5729166666666666),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (8, 0.5760416666666666),
[2025-02-10 15:55:01,460][flwr][INFO] - 	                        (9, 0.5854166666666666),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (10, 0.5822916666666667),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (11, 0.5666666666666667),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (12, 0.54375),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (13, 0.5354166666666667),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (14, 0.5156249999999999),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (15, 0.49270833333333325),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (16, 0.48749999999999993),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (17, 0.4739583333333333),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (18, 0.4697916666666667),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (19, 0.4604166666666667),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                        (20, 0.45)],
[2025-02-10 15:55:01,461][flwr][INFO] - 	 'clients_losses': [(1, 0.07601131101449332),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                    (2, 0.07467826555172602),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                    (3, 0.07195077960689863),
[2025-02-10 15:55:01,461][flwr][INFO] - 	                    (4, 0.06655760382612545),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (5, 0.058172297105193144),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (6, 0.048887712632616355),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (7, 0.04282930462310712),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (8, 0.042815215326845645),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (9, 0.048677815869450566),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (10, 0.060247592255473145),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (11, 0.07868031815936169),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (12, 0.10755089869101842),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (13, 0.14107166578372318),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (14, 0.18924244319399197),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (15, 0.25119172806541123),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (16, 0.3287573779622714),
[2025-02-10 15:55:01,462][flwr][INFO] - 	                    (17, 0.4254591777920722),
[2025-02-10 15:55:01,463][flwr][INFO] - 	                    (18, 0.5449563483397165),
[2025-02-10 15:55:01,463][flwr][INFO] - 	                    (19, 0.6896040717760721),
[2025-02-10 15:55:01,463][flwr][INFO] - 	                    (20, 0.8627415070931117)]}
[2025-02-10 15:55:01,463][flwr][INFO] - 	History (metrics, centralized):
[2025-02-10 15:55:01,463][flwr][INFO] - 	{'accuracy': [(0, 0.0822),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (1, 0.1349),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (2, 0.1302),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (3, 0.2749),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (4, 0.4119),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (5, 0.4769),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (6, 0.5355),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (7, 0.5647),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (8, 0.5795),
[2025-02-10 15:55:01,463][flwr][INFO] - 	              (9, 0.5877),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (10, 0.5879),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (11, 0.5844),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (12, 0.5753),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (13, 0.5587),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (14, 0.5421),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (15, 0.5254),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (16, 0.5135),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (17, 0.4942),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (18, 0.4782),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (19, 0.4627),
[2025-02-10 15:55:01,464][flwr][INFO] - 	              (20, 0.4491)]}
[2025-02-10 15:55:01,464][flwr][INFO] - 
